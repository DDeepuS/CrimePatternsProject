{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "import altair as alt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.patches import Patch\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score, root_mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "import xgboost as xgb\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.base import clone\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from prettytable import PrettyTable, TableStyle\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data, perform imputation and train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(option: str = \"1985-2023\") -> pd.DataFrame:\n",
    "    df = pd.DataFrame()\n",
    "    if option == \"1985-2023\":\n",
    "        df = pd.read_excel(\"../data/tentative_final_with_NaN.xlsx\")\n",
    "    if option == \"2010-2023\":\n",
    "        df = pd.read_excel(\"../data/tentative_final.xlsx\")\n",
    "    return df\n",
    "\n",
    "tentative_df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>crime_rate</th>\n",
       "      <th>Population</th>\n",
       "      <th>clearance_rate</th>\n",
       "      <th>population_density</th>\n",
       "      <th>unemployment_rate</th>\n",
       "      <th>adjusted_income</th>\n",
       "      <th>poverty_rate</th>\n",
       "      <th>rent_burden</th>\n",
       "      <th>home_ownership_rate</th>\n",
       "      <th>mobile_home_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>house_affordability</th>\n",
       "      <th>uninsured_rate</th>\n",
       "      <th>high_school_rate</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>public_school_rate</th>\n",
       "      <th>social_vs_security</th>\n",
       "      <th>security_vs_social</th>\n",
       "      <th>Category_Rural</th>\n",
       "      <th>Category_Suburban</th>\n",
       "      <th>Category_Urban</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>County</th>\n",
       "      <th>Year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Alameda</th>\n",
       "      <th>1985</th>\n",
       "      <td>0.009809</td>\n",
       "      <td>1185500</td>\n",
       "      <td>0.466890</td>\n",
       "      <td>1606.368564</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>0.010353</td>\n",
       "      <td>1206900</td>\n",
       "      <td>0.445778</td>\n",
       "      <td>1635.365854</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>0.009588</td>\n",
       "      <td>1220600</td>\n",
       "      <td>0.538580</td>\n",
       "      <td>1653.929539</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>0.008825</td>\n",
       "      <td>1242300</td>\n",
       "      <td>0.520660</td>\n",
       "      <td>1683.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>0.008375</td>\n",
       "      <td>1261200</td>\n",
       "      <td>0.497018</td>\n",
       "      <td>1708.943089</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              crime_rate  Population  clearance_rate  population_density  \\\n",
       "County  Year                                                               \n",
       "Alameda 1985    0.009809     1185500        0.466890         1606.368564   \n",
       "        1986    0.010353     1206900        0.445778         1635.365854   \n",
       "        1987    0.009588     1220600        0.538580         1653.929539   \n",
       "        1988    0.008825     1242300        0.520660         1683.333333   \n",
       "        1989    0.008375     1261200        0.497018         1708.943089   \n",
       "\n",
       "              unemployment_rate  adjusted_income  poverty_rate  rent_burden  \\\n",
       "County  Year                                                                  \n",
       "Alameda 1985                NaN              NaN           NaN          NaN   \n",
       "        1986                NaN              NaN           NaN          NaN   \n",
       "        1987                NaN              NaN           NaN          NaN   \n",
       "        1988                NaN              NaN           NaN          NaN   \n",
       "        1989                NaN              NaN           NaN          NaN   \n",
       "\n",
       "              home_ownership_rate  mobile_home_ratio  ...  \\\n",
       "County  Year                                          ...   \n",
       "Alameda 1985                  NaN                NaN  ...   \n",
       "        1986                  NaN                NaN  ...   \n",
       "        1987                  NaN                NaN  ...   \n",
       "        1988                  NaN                NaN  ...   \n",
       "        1989                  NaN                NaN  ...   \n",
       "\n",
       "              house_affordability  uninsured_rate  high_school_rate  \\\n",
       "County  Year                                                          \n",
       "Alameda 1985                  NaN             NaN               NaN   \n",
       "        1986                  NaN             NaN               NaN   \n",
       "        1987                  NaN             NaN               NaN   \n",
       "        1988                  NaN             NaN               NaN   \n",
       "        1989                  NaN             NaN               NaN   \n",
       "\n",
       "              dropout_rate  public_school_rate  social_vs_security  \\\n",
       "County  Year                                                         \n",
       "Alameda 1985           NaN                 NaN                 NaN   \n",
       "        1986           NaN                 NaN                 NaN   \n",
       "        1987           NaN                 NaN                 NaN   \n",
       "        1988           NaN                 NaN                 NaN   \n",
       "        1989           NaN                 NaN                 NaN   \n",
       "\n",
       "              security_vs_social  Category_Rural  Category_Suburban  \\\n",
       "County  Year                                                          \n",
       "Alameda 1985                 NaN               0                  0   \n",
       "        1986                 NaN               0                  0   \n",
       "        1987                 NaN               0                  0   \n",
       "        1988                 NaN               0                  0   \n",
       "        1989                 NaN               0                  0   \n",
       "\n",
       "              Category_Urban  \n",
       "County  Year                  \n",
       "Alameda 1985               1  \n",
       "        1986               1  \n",
       "        1987               1  \n",
       "        1988               1  \n",
       "        1989               1  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tentative_df[\"social_vs_security\"]=(tentative_df['adj_education_budget']+tentative_df['adj_welfare_budget']+tentative_df['adj_health_budget'])/(tentative_df['adj_police_budget']+tentative_df['adj_judiciary_budget']+tentative_df['adj_prison_budget'])\n",
    "tentative_df[\"security_vs_social\"]=(tentative_df['adj_police_budget']+tentative_df['adj_judiciary_budget']+tentative_df['adj_prison_budget'])/(tentative_df['adj_education_budget']+tentative_df['adj_welfare_budget']+tentative_df['adj_health_budget'])\n",
    "feature_0=['Population','clearance_rate',\n",
    "       'population_density', 'unemployment_rate', 'adjusted_income',\n",
    "       'poverty_rate', 'rent_burden', 'home_ownership_rate',\n",
    "       'mobile_home_ratio', 'vacancy_rate', 'Number_of_Persons_per_HseHld',\n",
    "       'renter_ratio', 'median_age', 'adj_police_budget',\n",
    "       'adj_education_budget', 'adj_welfare_budget',\n",
    "       'adj_mental_health_budget', 'adj_rehab_budget', 'adj_health_budget',\n",
    "       'adj_judiciary_budget', 'adj_prison_budget', 'median_house_value',\n",
    "       'house_affordability', 'uninsured_rate',\n",
    "       'high_school_rate', 'dropout_rate', 'public_school_rate',\n",
    "        \"social_vs_security\", \"security_vs_social\"] #'adherent_rate', 'rdm',\n",
    "feature_cat=['Category_Rural', 'Category_Suburban', 'Category_Urban']\n",
    "crime_dataframe=tentative_df[['County', 'Year',  'crime_rate']+feature_0+feature_cat]\n",
    "crime_dataframe=crime_dataframe.set_index(['County', 'Year'])\n",
    "crime_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 2262 entries, ('Alameda', 1985) to ('Yuba', 2023)\n",
      "Data columns (total 33 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   crime_rate                    2262 non-null   float64\n",
      " 1   Population                    2262 non-null   int64  \n",
      " 2   clearance_rate                2262 non-null   float64\n",
      " 3   population_density            2262 non-null   float64\n",
      " 4   unemployment_rate             1914 non-null   float64\n",
      " 5   adjusted_income               928 non-null    float64\n",
      " 6   poverty_rate                  870 non-null    float64\n",
      " 7   rent_burden                   812 non-null    float64\n",
      " 8   home_ownership_rate           812 non-null    float64\n",
      " 9   mobile_home_ratio             812 non-null    float64\n",
      " 10  vacancy_rate                  812 non-null    float64\n",
      " 11  Number_of_Persons_per_HseHld  812 non-null    float64\n",
      " 12  renter_ratio                  812 non-null    float64\n",
      " 13  median_age                    812 non-null    float64\n",
      " 14  adj_police_budget             852 non-null    float64\n",
      " 15  adj_education_budget          844 non-null    float64\n",
      " 16  adj_welfare_budget            852 non-null    float64\n",
      " 17  adj_mental_health_budget      834 non-null    float64\n",
      " 18  adj_rehab_budget              779 non-null    float64\n",
      " 19  adj_health_budget             852 non-null    float64\n",
      " 20  adj_judiciary_budget          852 non-null    float64\n",
      " 21  adj_prison_budget             852 non-null    float64\n",
      " 22  median_house_value            798 non-null    float64\n",
      " 23  house_affordability           798 non-null    float64\n",
      " 24  uninsured_rate                776 non-null    float64\n",
      " 25  high_school_rate              812 non-null    float64\n",
      " 26  dropout_rate                  754 non-null    float64\n",
      " 27  public_school_rate            754 non-null    float64\n",
      " 28  social_vs_security            844 non-null    float64\n",
      " 29  security_vs_social            844 non-null    float64\n",
      " 30  Category_Rural                2262 non-null   int64  \n",
      " 31  Category_Suburban             2262 non-null   int64  \n",
      " 32  Category_Urban                2262 non-null   int64  \n",
      "dtypes: float64(29), int64(4)\n",
      "memory usage: 591.5+ KB\n"
     ]
    }
   ],
   "source": [
    "crime_dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(624, 33)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final={}\n",
    "Final[\"dropna\"]=crime_dataframe.dropna()\n",
    "Final[\"dropna\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Copy your original DataFrame and set a MultiIndex\n",
    "df = crime_dataframe.copy()\n",
    "#df = df.set_index(['County', 'Year'])\n",
    "\n",
    "def fill_missing_with_linear_regression(group):\n",
    "    \"\"\"\n",
    "    For a single county (group), fit a simple linear regression model\n",
    "    Year vs. each numeric column. Use that model to fill missing values.\n",
    "    \"\"\"\n",
    "    # Sort by Year for clarity\n",
    "    group = group.sort_index(level='Year')\n",
    "    \n",
    "    # Iterate over each column\n",
    "    for col in group.columns:\n",
    "        # Only process numeric columns\n",
    "        if pd.api.types.is_numeric_dtype(group[col]):\n",
    "            # Extract the known data points (drop missing)\n",
    "            valid_data = group[col].dropna()\n",
    "            \n",
    "            # If there aren't at least two valid points, we can't fit a regression\n",
    "            if len(valid_data) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Prepare X (Year) and y (column values)\n",
    "            X = valid_data.index.get_level_values('Year').values.reshape(-1, 1)\n",
    "            y = valid_data.values\n",
    "            \n",
    "            # Fit the linear regression model\n",
    "            model = LinearRegression().fit(X, y)\n",
    "            \n",
    "            # Predict for all years in this county\n",
    "            X_all = group.index.get_level_values('Year').values.reshape(-1, 1)\n",
    "            y_pred = model.predict(X_all)\n",
    "            \n",
    "            # Fill only missing values with the predictions\n",
    "            missing_mask = group[col].isna()\n",
    "            group.loc[missing_mask, col] = y_pred[missing_mask]\n",
    "        else:\n",
    "            print(f\"Skipping non-numeric column: {col}\")\n",
    "    \n",
    "    return group\n",
    "\n",
    "# 2. Group by County and apply the regression-based filling\n",
    "df_reg_filled = (\n",
    "    df.groupby(level='County', group_keys=False)\n",
    "      .apply(fill_missing_with_linear_regression)\n",
    ")\n",
    "\n",
    "#df_reg_filled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2262, 33)\n",
      "(2108, 33)\n"
     ]
    }
   ],
   "source": [
    "final_dataframe = df_reg_filled\n",
    "#print(final_dataframe.isna().sum())\n",
    "print(final_dataframe.shape)\n",
    "final_dataframe=final_dataframe.dropna()\n",
    "print(final_dataframe.shape)\n",
    "Final[\"imputed\"]=final_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.163726529166426e-06\n",
      "Average variance of crime_rate by county: 3.7796157807726586e-06\n",
      "Average variance of crime_rate by year: 5.260953083223783e-06\n"
     ]
    }
   ],
   "source": [
    "print(np.var(final_dataframe['crime_rate']))\n",
    "# Group by county and compute the variance of crime_rate for each county,\n",
    "# then compute the average variance across all counties.\n",
    "county_variances = final_dataframe.groupby(level=\"County\")[\"crime_rate\"].var()\n",
    "avg_county_variance = county_variances.mean()\n",
    "print(\"Average variance of crime_rate by county:\", avg_county_variance)\n",
    "\n",
    "# Group by year and compute the variance of crime_rate for each year,\n",
    "# then compute the average variance across all years.\n",
    "year_variances = final_dataframe.groupby(level=\"Year\")[\"crime_rate\"].var()\n",
    "avg_year_variance = year_variances.mean()\n",
    "print(\"Average variance of crime_rate by year:\", avg_year_variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Poly_df_function(df0):\n",
    "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    Poly_data = poly.fit_transform(df0)\n",
    "    feature_names = poly.get_feature_names_out(input_features=df0.columns)\n",
    "    Poly_df = pd.DataFrame(Poly_data, columns=feature_names, index=df0.index)\n",
    "    return Poly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For these dictionaries, keys are dropna/imputed, All/Urban/Suburban/Rural, and values are the corresponding DataFrames\n",
    "X_train_dt={}\n",
    "X_test_dt={}\n",
    "y_train_dt={}\n",
    "y_test_dt={}\n",
    "X_train_Poly_dt={}\n",
    "X_test_Poly_dt={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in Final.keys():\n",
    "    for key1 in [\"All\", \"Urban\", \"Suburban\", \"Rural\"]:\n",
    "        if key1 == \"All\":\n",
    "            df = Final[key]\n",
    "        else:\n",
    "            df = Final[key][Final[key][\"Category_\"+key1] == 1]\n",
    "\n",
    "        # Split the data into features and target\n",
    "        X = df[feature_0]\n",
    "        y = df[\"crime_rate\"]\n",
    "        PolyX=Poly_df_function(X)\n",
    "        \n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test, X_train_Poly, X_test_Poly = train_test_split(X, y, PolyX, test_size=0.15, random_state=35,shuffle=True)\n",
    "\n",
    "        # Store the results in the dictionaries\n",
    "        X_train_dt[(key,key1)] = X_train\n",
    "        X_test_dt[(key,key1)] = X_test\n",
    "        y_train_dt[(key,key1)] = y_train\n",
    "        y_test_dt[(key,key1)] = y_test\n",
    "        X_train_Poly_dt[(key,key1)] = X_train_Poly\n",
    "        X_test_Poly_dt[(key,key1)] = X_test_Poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not using this\n",
    "\n",
    "def train_test_split_by_county(X, y, X_poly, test_size=0.15, random_state=42):\n",
    "    # Extract unique counties from the MultiIndex (assumed to have level \"County\")\n",
    "    counties = np.array(X.index.get_level_values(\"County\").unique())\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Shuffle and split counties\n",
    "    n_counties = len(counties)\n",
    "    n_test = int(np.floor(test_size * n_counties))\n",
    "    shuffled = np.random.permutation(counties)\n",
    "    test_counties = shuffled[:n_test]\n",
    "    train_counties = shuffled[n_test:]\n",
    "    \n",
    "    # Create masks for train and test based on counties\n",
    "    train_mask = X.index.get_level_values(\"County\").isin(train_counties)\n",
    "    test_mask = X.index.get_level_values(\"County\").isin(test_counties)\n",
    "    \n",
    "    # Subset the data using the masks\n",
    "    X_train = X[train_mask]\n",
    "    X_test  = X[test_mask]\n",
    "    y_train = y[train_mask]\n",
    "    y_test  = y[test_mask]\n",
    "    X_poly_train = X_poly[train_mask]\n",
    "    X_poly_test  = X_poly[test_mask]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, X_poly_train, X_poly_test\n",
    "\n",
    "# Now apply the function for each dataset:\n",
    "#X_train_Urban, X_test_Urban, y_train_Urban, y_test_Urban, X_train_Poly_Urban, X_test_Poly_Urban = train_test_split_by_county(\n",
    "#    X_Urban, y_Urban, X_Poly_Urban, test_size=0.15, random_state=42\n",
    "#)\n",
    "\n",
    "#X_train_Suburban, X_test_Suburban, y_train_Suburban, y_test_Suburban, X_train_Poly_Suburban, X_test_Poly_Suburban = train_test_split_by_county(\n",
    "#    X_Suburban, y_Suburban, X_Poly_Suburban, test_size=0.15, random_state=42\n",
    "#)\n",
    "\n",
    "#X_train_Rural, X_test_Rural, y_train_Rural, y_test_Rural, X_train_Poly_Rural, X_test_Poly_Rural = train_test_split_by_county(\n",
    "#    X_Rural, y_Rural, X_Poly_Rural, test_size=0.15, random_state=42\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_train_Urban.index.get_level_values('County').unique())\n",
    "#print(X_test_Urban.index.get_level_values('County').unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_table(model_stats):\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Model\", \"Type\", \"MSE\", \"RMSE\", \"r2 Score\", \"MR2\"]\n",
    "    \n",
    "    for model_name, stats in model_stats.items():\n",
    "        first_row = True\n",
    "        for cv_type, metrics in stats.items():\n",
    "            if len(metrics) == 3:\n",
    "                mse, rmse, r2 = metrics\n",
    "                mr2 = \"\"\n",
    "            elif len(metrics) == 4:\n",
    "                mse, rmse, r2, mr2 = metrics\n",
    "            else:\n",
    "                mse, rmse, r2, mr2 = metrics[0], metrics[1], metrics[2], metrics[3] if len(metrics) > 3 else \"\"\n",
    "            \n",
    "            if first_row:\n",
    "                table.add_row([model_name, cv_type, mse, rmse, r2, mr2])\n",
    "                first_row = False\n",
    "            else:\n",
    "                table.add_row([\"\", cv_type, mse, rmse, r2, mr2])\n",
    "        # Divider row between models\n",
    "        table.add_row([\"\"] * 6)\n",
    "    \n",
    "    table.set_style(TableStyle.DOUBLE_BORDER)\n",
    "    print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_PCA(fitted_models, features, size, plot=True, models_to_use=None):\n",
    "    \"\"\"\n",
    "    Compute and optionally plot feature importances (or effective coefficients) for different models.\n",
    "    \n",
    "    Parameters:\n",
    "      fitted_models (dict): Dictionary containing fitted models.\n",
    "      features (list): List of original feature names.\n",
    "      size (tuple): Figure size for the plots.\n",
    "      plot (bool): If True, plot the bar charts; if False, only print the top features.\n",
    "      models_to_use (iterable): List or set of model names to consider. \n",
    "                                Defaults to all [\"mlr\", \"ridge\", \"xgb\", \"random_forest\"].\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    if models_to_use is None:\n",
    "        models_to_use = {\"mlr\", \"ridge\", \"xgb\", \"random_forest\"}\n",
    "    else:\n",
    "        models_to_use = set(models_to_use)\n",
    "        \n",
    "    # For MLR (Linear Regression) with PCA\n",
    "    if \"mlr\" in models_to_use:\n",
    "        scaler_mlr = fitted_models[\"mlr\"].named_steps[\"scale\"]\n",
    "        pca_mlr = fitted_models[\"mlr\"].named_steps[\"pca\"]\n",
    "        lin_mlr = fitted_models[\"mlr\"].named_steps[\"Lin\"]\n",
    "        # Back-transform coefficients: effective_coef = (PCA.components_.T @ coef) / scaler.scale_\n",
    "        eff_coef_mlr = (pca_mlr.components_.T.dot(lin_mlr.coef_)) / scaler_mlr.scale_\n",
    "        sorted_indices_mlr = np.argsort(eff_coef_mlr)\n",
    "        if plot:\n",
    "            plt.figure(figsize=size)\n",
    "            plt.barh([features[i] for i in sorted_indices_mlr], eff_coef_mlr[sorted_indices_mlr])\n",
    "            plt.title(\"MLR Effective Coefficients (Original Features)\")\n",
    "            plt.xlabel(\"Coefficient\")\n",
    "            plt.show()\n",
    "        ll = [(abs(eff_coef_mlr[i]), features[i]) for i in sorted_indices_mlr]\n",
    "        ll.sort(reverse=True)\n",
    "        print(\"The top 4 important features in mlr are:\")\n",
    "        for importance, feat in ll[:4]:\n",
    "            print(feat)\n",
    "    \n",
    "    # For Ridge Regression with PCA\n",
    "    if \"ridge\" in models_to_use:\n",
    "        scaler_ridge = fitted_models[\"ridge\"].named_steps[\"scale\"]\n",
    "        pca_ridge = fitted_models[\"ridge\"].named_steps[\"pca\"]\n",
    "        ridge_model = fitted_models[\"ridge\"].named_steps[\"ridge\"]\n",
    "        eff_coef_ridge = (pca_ridge.components_.T.dot(ridge_model.coef_)) / scaler_ridge.scale_\n",
    "        sorted_indices_ridge = np.argsort(eff_coef_ridge)\n",
    "        if plot:\n",
    "            plt.figure(figsize=size)\n",
    "            plt.barh([features[i] for i in sorted_indices_ridge], eff_coef_ridge[sorted_indices_ridge])\n",
    "            plt.title(\"Ridge Effective Coefficients (Original Features)\")\n",
    "            plt.xlabel(\"Coefficient\")\n",
    "            plt.show()\n",
    "        ll = [(abs(eff_coef_ridge[i]), features[i]) for i in sorted_indices_ridge]\n",
    "        ll.sort(reverse=True)\n",
    "        print(\"The top 4 important features in ridge are:\")\n",
    "        for importance, feat in ll[:4]:\n",
    "            print(feat)\n",
    "    \n",
    "    # For XGB (assumed to be trained on original features)\n",
    "    if \"xgb\" in models_to_use:\n",
    "        importance_xgb = fitted_models[\"xgb\"].feature_importances_\n",
    "        sorted_indices_xgb = np.argsort(importance_xgb)\n",
    "        if plot:\n",
    "            plt.figure(figsize=size)\n",
    "            plt.barh([features[i] for i in sorted_indices_xgb], importance_xgb[sorted_indices_xgb])\n",
    "            plt.title(\"XGB Feature Importance\")\n",
    "            plt.xlabel(\"Importance\")\n",
    "            plt.show()\n",
    "        ll = [(importance_xgb[i], features[i]) for i in sorted_indices_xgb]\n",
    "        ll.sort(reverse=True)\n",
    "        print(\"The top 4 important features in xgb are:\")\n",
    "        for imp, feat in ll[:4]:\n",
    "            print(feat)\n",
    "    \n",
    "    # For RandomForest with PCA in a pipeline:\n",
    "    if \"random_forest\" in models_to_use:\n",
    "        scaler_rf = fitted_models[\"random_forest\"].named_steps[\"scale\"]\n",
    "        pca_rf = fitted_models[\"random_forest\"].named_steps[\"pca\"]\n",
    "        rf_model = fitted_models[\"random_forest\"].named_steps[\"randomforest\"]\n",
    "        # Heuristic: effective importance = abs(PCA.components_.T) dot tree_importances\n",
    "        eff_importance_rf = np.abs(pca_rf.components_.T).dot(rf_model.feature_importances_)\n",
    "        sorted_indices_rf = np.argsort(eff_importance_rf)\n",
    "        if plot:\n",
    "            plt.figure(figsize=size)\n",
    "            plt.barh([features[i] for i in sorted_indices_rf], eff_importance_rf[sorted_indices_rf])\n",
    "            plt.title(\"RandomForest Effective Feature Importance (Original Features)\")\n",
    "            plt.xlabel(\"Importance\")\n",
    "            plt.show()\n",
    "        ll = [(eff_importance_rf[i], features[i]) for i in sorted_indices_rf]\n",
    "        ll.sort(reverse=True)\n",
    "        print(\"The top 4 important features in random forest are:\")\n",
    "        for imp, feat in ll[:4]:\n",
    "            print(feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_MR2(S, X, y, model):\n",
    "    \"\"\"\n",
    "    Computes the average modified R² (MR2) over leave-one-county-out splits.\n",
    "    \n",
    "    In each fold, MR2 is defined as:\n",
    "         MR2 = 1 - (MSE of model predictions on validation set) / (Baseline MSE),\n",
    "    where Baseline MSE is computed by predicting the mean of y in the training set for that fold.\n",
    "         \n",
    "    Parameters:\n",
    "      S : set\n",
    "          Set of feature names.\n",
    "      X : pd.DataFrame\n",
    "          DataFrame with a MultiIndex (County, Year) containing features.\n",
    "      y : pd.Series or pd.DataFrame\n",
    "          Target values.\n",
    "      model : scikit-learn estimator\n",
    "          The model to use for evaluation.\n",
    "    \n",
    "    Returns:\n",
    "      float\n",
    "          The average MR2 over all LOCO splits.\n",
    "    \"\"\"\n",
    "    if not S:\n",
    "        return -np.inf  # if no features, MR2 is worst\n",
    "    logo = LeaveOneGroupOut()\n",
    "    groups = X.index.get_level_values(\"County\")\n",
    "    mr2_scores = []\n",
    "    XX = X[list(S)]\n",
    "    \n",
    "    for train_idx, val_idx in logo.split(X, y, groups):\n",
    "        X_tt, X_val = XX.iloc[train_idx], XX.iloc[val_idx]\n",
    "        y_tt, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        model_clone = clone(model)\n",
    "        model_clone.fit(X_tt, y_tt)\n",
    "        y_pred = model_clone.predict(X_val)\n",
    "        \n",
    "        mse_model = mean_squared_error(y_val, y_pred)\n",
    "        # Baseline: predict mean of y_tt for all validation instances.\n",
    "        baseline_pred = np.mean(y_tt)\n",
    "        baseline_y = np.full(shape=y_val.shape, fill_value=baseline_pred)\n",
    "        mse_baseline = mean_squared_error(y_val, baseline_y)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if mse_baseline == 0:\n",
    "            mr2 = -np.inf\n",
    "        else:\n",
    "            mr2 = 1 - mse_model / mse_baseline\n",
    "        mr2_scores.append(mr2)\n",
    "    \n",
    "    return np.mean(mr2_scores)\n",
    "\n",
    "def compute_avg_MR2_fast(S, X, y, model, n_splits=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Computes the average modified R² (MR2) over folds where the counties are divided\n",
    "    into n_splits groups. In each fold, the model is trained on four of the groups (counties)\n",
    "    and validated on the remaining group.\n",
    "    \n",
    "    MR2 is defined as:\n",
    "         MR2 = 1 - (MSE of model predictions on validation set)/(Baseline MSE),\n",
    "    where Baseline MSE is computed by predicting the mean of y on the training counties.\n",
    "    \n",
    "    Parameters:\n",
    "      S : set\n",
    "          Set of feature names.\n",
    "      X : pd.DataFrame\n",
    "          DataFrame with a MultiIndex (County, Year) containing features.\n",
    "      y : pd.Series or pd.DataFrame\n",
    "          Target values.\n",
    "      model : scikit-learn estimator\n",
    "          The model (or pipeline) to use for evaluation.\n",
    "      n_splits : int, default=5\n",
    "          The number of splits (groups) to create from the unique counties.\n",
    "      random_state : int, default=42\n",
    "          For reproducibility when shuffling counties.\n",
    "    \n",
    "    Returns:\n",
    "      float\n",
    "          The average MR2 over all folds.\n",
    "    \"\"\"\n",
    "    if not S:\n",
    "        return -np.inf  # if no features, MR2 is worst\n",
    "\n",
    "    # Extract unique counties\n",
    "    counties = np.array(X.index.get_level_values(\"County\").unique())\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    mr2_scores = []\n",
    "    XX = X[list(S)]\n",
    "    \n",
    "    for train_county_idx, val_county_idx in kf.split(counties):\n",
    "        train_counties = counties[train_county_idx]\n",
    "        val_counties = counties[val_county_idx]\n",
    "        \n",
    "        # Generate boolean masks for training and validation samples based on counties\n",
    "        train_mask = X.index.get_level_values(\"County\").isin(train_counties)\n",
    "        val_mask   = X.index.get_level_values(\"County\").isin(val_counties)\n",
    "        \n",
    "        X_tt, X_val = XX[train_mask], XX[val_mask]\n",
    "        y_tt, y_val = y[train_mask], y[val_mask]\n",
    "        \n",
    "        # Clone and fit the model on the current training fold\n",
    "        model_clone = clone(model)\n",
    "        model_clone.fit(X_tt, y_tt)\n",
    "        y_pred = model_clone.predict(X_val)\n",
    "        mse_model = mean_squared_error(y_val, y_pred)\n",
    "        \n",
    "        # Baseline: predict the mean of y in the training fold\n",
    "        baseline_pred = np.mean(y_tt)\n",
    "        baseline_y = np.full(shape=y_val.shape, fill_value=baseline_pred)\n",
    "        mse_baseline = mean_squared_error(y_val, baseline_y)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if mse_baseline == 0:\n",
    "            mr2 = -np.inf\n",
    "        else:\n",
    "            mr2 = 1 - mse_model / mse_baseline\n",
    "        mr2_scores.append(mr2)\n",
    "        \n",
    "    return np.mean(mr2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_features(S, X, y, model, threshold=0.005):\n",
    "    \"\"\"\n",
    "    Recursively removes one feature at a time from set S if doing so does not reduce the average MR2 \n",
    "    by more than the given threshold.\n",
    "    \n",
    "    Uses DFS to search through the removal space and employs a fast version of MR2 computation \n",
    "    (which divides the counties into groups instead of leaving one county out at a time).\n",
    "    \n",
    "    Parameters:\n",
    "      S : set\n",
    "          Current set of features.\n",
    "      X : pd.DataFrame\n",
    "          DataFrame with a MultiIndex (County, Year) containing features.\n",
    "      y : pd.Series or pd.DataFrame\n",
    "          Target values.\n",
    "      model : scikit-learn estimator\n",
    "          The model to use for evaluation.\n",
    "      threshold : float, default=0.005\n",
    "          The maximum allowed drop in average MR2 for a feature to be removed.\n",
    "          \n",
    "    Returns:\n",
    "      set\n",
    "          The subset of S that yields an average MR2 that is not reduced by more than the threshold.\n",
    "    \"\"\"\n",
    "    current_avg = compute_avg_MR2_fast(S, X, y, model)\n",
    "    best = S.copy()\n",
    "    \n",
    "    for feat in list(S):\n",
    "        new_set = S - {feat}\n",
    "        if not new_set:  # skip if empty\n",
    "            continue\n",
    "        new_avg = compute_avg_MR2_fast(new_set, X, y, model)\n",
    "        # Accept removal if the drop in average MR2 is less than or equal to threshold.\n",
    "        if new_avg >= current_avg - threshold:\n",
    "            candidate = remove_features(new_set, X, y, model, threshold)\n",
    "            candidate_avg = compute_avg_MR2_fast(candidate, X, y, model)\n",
    "            if candidate_avg >= current_avg - threshold:\n",
    "                best = candidate\n",
    "                current_avg = candidate_avg\n",
    "    return best\n",
    "\n",
    "def add_features(S1, S2, X, y, model, threshold=0.005):\n",
    "    \"\"\"\n",
    "    Recursively adds one feature at a time from S2 into S1 if doing so increases the average MR2 \n",
    "    by at least the specified threshold.\n",
    "    \n",
    "    Uses DFS to search through the addition space and employs a fast version of MR2 computation \n",
    "    (which divides the counties into groups instead of leaving one county out at a time).\n",
    "    \n",
    "    Parameters:\n",
    "      S1 : set\n",
    "          Current selected features.\n",
    "      S2 : set\n",
    "          Potential features to add.\n",
    "      X : pd.DataFrame\n",
    "          DataFrame with a MultiIndex (County, Year) containing features.\n",
    "      y : pd.Series or pd.DataFrame\n",
    "          Target values.\n",
    "      model : scikit-learn estimator\n",
    "          The model to use for evaluation.\n",
    "      threshold : float, default=0.005\n",
    "          The minimum increase in average MR2 required to add a new feature.\n",
    "      \n",
    "    Returns:\n",
    "      set\n",
    "          The augmented feature set that yields a higher average MR2.\n",
    "    \"\"\"\n",
    "    current_avg = compute_avg_MR2_fast(S1, X, y, model)\n",
    "    best = S1.copy()\n",
    "    for feat in list(S2):\n",
    "        new_set = S1 | {feat}\n",
    "        new_avg = compute_avg_MR2_fast(new_set, X, y, model)\n",
    "        if new_avg >= current_avg + threshold:\n",
    "            remaining = S2 - {feat}\n",
    "            candidate = add_features(new_set, remaining, X, y, model, threshold)\n",
    "            candidate_avg = compute_avg_MR2_fast(candidate, X, y, model)\n",
    "            if candidate_avg >= current_avg + threshold:\n",
    "                best = candidate\n",
    "                current_avg = candidate_avg\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Poly_feature_selection(S, X, y, model, threshold_addition=0.002, threshold_removal=0.002, dd=25):\n",
    "    \"\"\"\n",
    "    Performs feature selection on a large set of polynomial features S by gradually adding\n",
    "    features in blocks and refining via removal.\n",
    "    \n",
    "    The procedure is:\n",
    "      1) Initialize S0 as the base set 'feature_0' and remove any non-contributing features.\n",
    "      2) Iterate over S in blocks of dd features. For each block:\n",
    "            - Use add_features (with threshold_addition) to add the block features to S0.\n",
    "            - Then, use remove_features (with threshold_removal) to remove features that do not contribute.\n",
    "      3) After processing all blocks, add back feature_0 using add_features and then run one final removal.\n",
    "    \n",
    "    Parameters:\n",
    "      S : iterable\n",
    "          The large set of candidate feature names (e.g. from polynomial expansion).\n",
    "      X : pd.DataFrame\n",
    "          DataFrame with features (MultiIndex with levels \"County\" and \"Year\").\n",
    "      y : pd.Series or pd.DataFrame\n",
    "          The target values.\n",
    "      model : scikit-learn estimator\n",
    "          The model (or pipeline) to use for evaluation.\n",
    "      threshold_addition : float, default=0.002\n",
    "          The minimum increase in average MR2 required to add a new feature.\n",
    "      threshold_removal : float, default=0.002\n",
    "          The maximum permitted drop in average MR2 when removing a feature.\n",
    "      dd : int, default=25\n",
    "          Block size to process features incrementally.\n",
    "          \n",
    "    Returns:\n",
    "      set\n",
    "          The final optimal subset of features.\n",
    "      \n",
    "    Note:\n",
    "      It is assumed that 'feature_0' is defined globally and represents your initial base features.\n",
    "    \"\"\"\n",
    "    ll = list(S)\n",
    "    N = len(ll)\n",
    "    \n",
    "    # Start with the base features, and remove any non-contributing features.\n",
    "    S0 = set(feature_0)\n",
    "    \n",
    "    # Process candidate features in blocks of dd.\n",
    "    for i in range(0, N, dd):\n",
    "        print(f\"Processing block {i} to {i+dd}\")\n",
    "        block = set(ll[i:i+dd])\n",
    "        # Use add_features (DFS) to try to add the features in this block.\n",
    "        S0 = add_features(S0, block, X, y, model, threshold=threshold_addition)\n",
    "        # Then run removal on the updated S0 to prune any redundant features.\n",
    "        S0 = remove_features(S0, X, y, model, threshold=threshold_removal)\n",
    "    \n",
    "    # After processing all blocks, ensure that the base features remain if beneficial.\n",
    "    S0 = add_features(S0, set(feature_0), X, y, model, threshold=0)\n",
    "    S0 = remove_features(S0, X, y, model, threshold=threshold_removal)\n",
    "    \n",
    "    return S0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def compute_MR2(y_true, y_pred, baseline_value):\n",
    "    \"\"\"\n",
    "    Computes the modified R² (MR2) metric given true values, model predictions,\n",
    "    and a baseline value. The baseline prediction is a constant equal to baseline_value.\n",
    "    \n",
    "    MR2 = 1 - (MSE(model predictions))/(MSE(baseline predictions))\n",
    "    \n",
    "    Parameters:\n",
    "      y_true : array-like\n",
    "          True target values.\n",
    "      y_pred : array-like\n",
    "          Predictions from the model.\n",
    "      baseline_value : float\n",
    "          The constant value used as baseline (typically the mean from the training fold).\n",
    "    \n",
    "    Returns:\n",
    "      float\n",
    "          The computed MR2. If the baseline MSE is zero, returns -np.inf.\n",
    "    \"\"\"\n",
    "    mse_model = mean_squared_error(y_true, y_pred)\n",
    "    baseline_pred = np.full(shape=y_true.shape, fill_value=baseline_value)\n",
    "    mse_baseline = mean_squared_error(y_true, baseline_pred)\n",
    "    if mse_baseline == 0:\n",
    "        return -np.inf\n",
    "    return 1 - mse_model / mse_baseline\n",
    "\n",
    "def deep_cross_fit1(\n",
    "    X_train, \n",
    "    X_test, \n",
    "    y_train, \n",
    "    y_test, \n",
    "    **models\n",
    ") -> tuple[dict, dict, dict]:\n",
    "    \"\"\"\n",
    "    Fit models and compute performance statistics using several cross-validation schemes:\n",
    "      - Regular (KFold) CV on the training set.\n",
    "      - County-based CV using leave-one-county-out (splitting by the 'County' level of the index).\n",
    "      - Time-series CV (using data until 2018 for training and data after 2018 for validation).\n",
    "      - Test set performance (after fitting on the full training data).\n",
    "    \n",
    "    Modified R² (MR2) is defined as:\n",
    "         MR2 = 1 - (MSE of model predictions)/(MSE of baseline predictions),\n",
    "    where the baseline predictions are computed using the mean of y from the training fold \n",
    "    (for CV) or the mean of y_train (for the test set).\n",
    "    \n",
    "    Parameters:\n",
    "      X_train (pd.DataFrame): Training features (MultiIndex with levels \"County\" and \"Year\")\n",
    "      X_test (pd.DataFrame): Test features (MultiIndex with levels \"County\" and \"Year\")\n",
    "      y_train (pd.DataFrame or pd.Series): Training targets (indexed similarly)\n",
    "      y_test (pd.DataFrame or pd.Series): Test targets\n",
    "      models: Keyword arguments mapping model names to scikit-learn estimator instances.\n",
    "      \n",
    "    Returns:\n",
    "      tuple:\n",
    "         fitted_models: dict mapping model name to fitted model (trained on X_train)\n",
    "         model_stats: dict mapping model name to a dictionary with keys:\n",
    "             \"Reg train\", \"Reg val\", \"County train\", \"County val\", \"Time train\", \"Time val\", \"Test\"\n",
    "             For each, the value is a list: [MSE, RMSE, r2, MR2]\n",
    "         predictions: dict mapping model name to a dict with keys:\n",
    "             \"train\": predictions on X_train,\n",
    "             \"test\": predictions on X_test.\n",
    "    \"\"\"\n",
    "    fitted_models = {}\n",
    "    model_stats = {}\n",
    "    predictions = {}\n",
    "    \n",
    "    # 1. Regular KFold CV on training data\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for key, model in models.items():\n",
    "        stats = {}\n",
    "        \n",
    "        # Initialize lists to store metrics for regular splits.\n",
    "        reg_train_mse, reg_train_rmse, reg_train_r2, reg_train_mr2 = [], [], [], []\n",
    "        reg_val_mse, reg_val_rmse, reg_val_r2, reg_val_mr2 = [], [], [], []\n",
    "        \n",
    "        for train_idx, val_idx in kf.split(X_train):\n",
    "            X_tt, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "            y_tt, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "            model_clone = clone(model)\n",
    "            model_clone.fit(X_tt, y_tt)\n",
    "            train_preds = model_clone.predict(X_tt)\n",
    "            val_preds = model_clone.predict(X_val)\n",
    "            \n",
    "            mse_train = mean_squared_error(y_tt, train_preds)\n",
    "            reg_train_mse.append(mse_train)\n",
    "            reg_train_rmse.append(root_mean_squared_error(y_tt, train_preds))\n",
    "            reg_train_r2.append(r2_score(y_tt, train_preds))\n",
    "            # Use compute_MR2 with baseline equal to mean of y_tt\n",
    "            reg_train_mr2.append(compute_MR2(y_tt, train_preds, np.mean(y_tt)))\n",
    "            \n",
    "            mse_val = mean_squared_error(y_val, val_preds)\n",
    "            reg_val_mse.append(mse_val)\n",
    "            reg_val_rmse.append(root_mean_squared_error(y_val, val_preds))\n",
    "            reg_val_r2.append(r2_score(y_val, val_preds))\n",
    "            # For validation, baseline is computed using the mean of y_tt from training fold.\n",
    "            reg_val_mr2.append(compute_MR2(y_val, val_preds, np.mean(y_tt)))\n",
    "            \n",
    "        stats[\"Reg train\"] = [np.mean(reg_train_mse), np.mean(reg_train_rmse), np.mean(reg_train_r2), np.mean(reg_train_mr2)]\n",
    "        stats[\"Reg val\"]   = [np.mean(reg_val_mse), np.mean(reg_val_rmse), np.mean(reg_val_r2), np.mean(reg_val_mr2)]\n",
    "        \n",
    "        # 2. County cross-validation using Leave-One-County-Out\n",
    "        logo = LeaveOneGroupOut()\n",
    "        groups = X_train.index.get_level_values(\"County\")\n",
    "        county_train_mse, county_train_rmse, county_train_r2, county_train_mr2 = [], [], [], []\n",
    "        county_val_mse, county_val_rmse, county_val_r2, county_val_mr2 = [], [], [], []\n",
    "        \n",
    "        for train_idx, val_idx in logo.split(X_train, y_train, groups):\n",
    "            X_train_county = X_train.iloc[train_idx]\n",
    "            y_train_county = y_train.iloc[train_idx]\n",
    "            X_val_county   = X_train.iloc[val_idx]\n",
    "            y_val_county   = y_train.iloc[val_idx]\n",
    "            model_clone = clone(model)\n",
    "            model_clone.fit(X_train_county, y_train_county)\n",
    "            train_preds = model_clone.predict(X_train_county)\n",
    "            val_preds   = model_clone.predict(X_val_county)\n",
    "            \n",
    "            mse_train = mean_squared_error(y_train_county, train_preds)\n",
    "            county_train_mse.append(mse_train)\n",
    "            county_train_rmse.append(root_mean_squared_error(y_train_county, train_preds))\n",
    "            if len(y_train_county) < 2:\n",
    "                county_train_r2.append(np.nan)\n",
    "                county_train_mr2.append(np.nan)\n",
    "            else:\n",
    "                county_train_r2.append(r2_score(y_train_county, train_preds))\n",
    "                county_train_mr2.append(compute_MR2(y_train_county, train_preds, np.mean(y_train_county)))\n",
    "            \n",
    "            mse_val = mean_squared_error(y_val_county, val_preds)\n",
    "            county_val_mse.append(mse_val)\n",
    "            county_val_rmse.append(root_mean_squared_error(y_val_county, val_preds))\n",
    "            if len(y_val_county) < 2:\n",
    "                county_val_r2.append(np.nan)\n",
    "                county_val_mr2.append(np.nan)\n",
    "            else:\n",
    "                county_val_r2.append(r2_score(y_val_county, val_preds))\n",
    "                # For validation, use training fold's mean\n",
    "                county_val_mr2.append(compute_MR2(y_val_county, val_preds, np.mean(y_train_county)))\n",
    "                \n",
    "        stats[\"County train\"] = [\n",
    "            np.nanmean(county_train_mse),\n",
    "            np.nanmean(county_train_rmse),\n",
    "            np.nanmean(county_train_r2),\n",
    "            np.nanmean(county_train_mr2)\n",
    "        ]\n",
    "        stats[\"County val\"] = [\n",
    "            np.nanmean(county_val_mse),\n",
    "            np.nanmean(county_val_rmse),\n",
    "            np.nanmean(county_val_r2),\n",
    "            np.nanmean(county_val_mr2)\n",
    "        ]\n",
    "        \n",
    "        # 3. Time series CV: train on years <= 2018, validate on years > 2018.\n",
    "        cutoff_year = 2018\n",
    "        train_mask = X_train.index.get_level_values(\"Year\") <= cutoff_year\n",
    "        val_mask   = X_train.index.get_level_values(\"Year\") > cutoff_year\n",
    "        \n",
    "        if train_mask.sum() > 0 and val_mask.sum() > 0:\n",
    "            X_train_time = X_train[train_mask]\n",
    "            y_train_time = y_train[train_mask]\n",
    "            X_val_time   = X_train[val_mask]\n",
    "            y_val_time   = y_train[val_mask]\n",
    "            model_clone = clone(model)\n",
    "            model_clone.fit(X_train_time, y_train_time)\n",
    "            train_preds = model_clone.predict(X_train_time)\n",
    "            val_preds   = model_clone.predict(X_val_time)\n",
    "            \n",
    "            mse_train = mean_squared_error(y_train_time, train_preds)\n",
    "            time_train_mse = mse_train\n",
    "            time_train_rmse = root_mean_squared_error(y_train_time, train_preds)\n",
    "            time_train_r2 = r2_score(y_train_time, train_preds)\n",
    "            time_train_mr2 = compute_MR2(y_train_time, train_preds, np.mean(y_train_time))\n",
    "            \n",
    "            mse_val = mean_squared_error(y_val_time, val_preds)\n",
    "            time_val_mse = mse_val\n",
    "            time_val_rmse = root_mean_squared_error(y_val_time, val_preds)\n",
    "            time_val_r2 = r2_score(y_val_time, val_preds)\n",
    "            # Baseline for validation uses training subset mean.\n",
    "            time_val_mr2 = compute_MR2(y_val_time, val_preds, np.mean(y_train_time))\n",
    "            \n",
    "            stats[\"Time train\"] = [time_train_mse, time_train_rmse, time_train_r2, time_train_mr2]\n",
    "            stats[\"Time val\"]   = [time_val_mse, time_val_rmse, time_val_r2, time_val_mr2]\n",
    "        else:\n",
    "            stats[\"Time train\"] = [np.nan, np.nan, np.nan, np.nan]\n",
    "            stats[\"Time val\"]   = [np.nan, np.nan, np.nan, np.nan]\n",
    "        \n",
    "        # 4. Test set evaluation: train on full X_train, then predict on X_test.\n",
    "        model.fit(X_train, y_train)\n",
    "        fitted_models[key] = model\n",
    "        test_preds = model.predict(X_test)\n",
    "        test_mse   = mean_squared_error(y_test, test_preds)\n",
    "        test_rmse  = root_mean_squared_error(y_test, test_preds)\n",
    "        test_r2    = r2_score(y_test, test_preds)\n",
    "        # For test set, the baseline is the constant prediction using the mean of y_train.\n",
    "        test_mr2   = compute_MR2(y_test, test_preds, np.mean(y_train))\n",
    "        stats[\"Test\"] = [test_mse, test_rmse, test_r2, test_mr2]\n",
    "        \n",
    "        model_stats[key] = stats\n",
    "        \n",
    "        # Also store predictions on training and test sets.\n",
    "        predictions[key] = {\n",
    "            \"train\": model.predict(X_train),\n",
    "            \"test\": test_preds\n",
    "        }\n",
    "        \n",
    "    return fitted_models, model_stats, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unused, extra codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Feature_selection(S, X, y, model, threshold_removal=0.005, threshold_addition=0.005):\n",
    "    \"\"\"\n",
    "    Selects an optimal subset of features using a three-step DFS process:\n",
    "      1) Removal: Remove features from S if doing so does not reduce MR2 by more than threshold_removal.\n",
    "      2) Addition: From the originally removed features, DFS to add features back if they improve MR2 by at least threshold_addition.\n",
    "      3) Final Removal: Run the removal process again on the augmented set using threshold_removal.\n",
    "    \n",
    "    Parameters:\n",
    "      S : iterable\n",
    "          The initial set (or list) of feature names.\n",
    "      X : pd.DataFrame\n",
    "          Training features with a MultiIndex (County, Year).\n",
    "      y : pd.Series or pd.DataFrame\n",
    "          Training target values.\n",
    "      model : scikit-learn estimator\n",
    "          The model to use for evaluation.\n",
    "      threshold_removal : float, default=0.005\n",
    "          The maximum permitted drop in average MR2 when removing a feature.\n",
    "      threshold_addition : float, default=0.005\n",
    "          The minimum increase in average MR2 required to add a new feature.\n",
    "    \n",
    "    Returns:\n",
    "      set\n",
    "          The final optimal subset of features.\n",
    "    \"\"\"\n",
    "    # Convert S to a set\n",
    "    S_initial = set(S)\n",
    "    # Step 1: Removal phase\n",
    "    S_removed = remove_features(S_initial, X, y, model, threshold=threshold_removal)\n",
    "    # Determine the features that were removed\n",
    "    removed_features = S_initial - S_removed\n",
    "    # Step 2: Addition phase\n",
    "    S_added = add_features(S_removed, removed_features, X, y, model, threshold=threshold_addition)\n",
    "    # Step 3: Final removal phase\n",
    "    S_final = remove_features(S_added, X, y, model, threshold=threshold_removal)\n",
    "    return S_final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def PCA_feature_selection(S, X, y, model, max_components=10):\n",
    "    \"\"\"\n",
    "    Finds the optimal number of PCA components and optimal feature subset.\n",
    "    \n",
    "    For each n_components (starting at 1), it creates a pipeline:\n",
    "         Pipeline([(\"scale\", StandardScaler()), (\"pca\", PCA(n_components=n_components)), (\"mod\", model)])\n",
    "    Then it calls Feature_selection(S, X, y, model_pipe) to select the best features\n",
    "    and computes the average MR2 using leave-one-county-out CV (via compute_avg_MR2).\n",
    "    \n",
    "    It increases n_components by 1 until the average MR2 decreases; then, the previous\n",
    "    setting is returned.\n",
    "    \n",
    "    Parameters:\n",
    "      S : iterable\n",
    "          The initial set of feature names.\n",
    "      X : pd.DataFrame\n",
    "          Training features with a MultiIndex (County, Year).\n",
    "      y : pd.Series or pd.DataFrame\n",
    "          Training target values.\n",
    "      model : scikit-learn estimator\n",
    "          The model to use (e.g., LinearRegression()).\n",
    "      max_components : int, optional\n",
    "          Maximum number of PCA components to try (default is 10).\n",
    "    \n",
    "    Returns:\n",
    "      tuple: (best_n_components, best_feature_subset, best_avg_MR2)\n",
    "    \"\"\"\n",
    "    best_mr2 = -np.inf\n",
    "    best_n = None\n",
    "    best_features = None\n",
    "    \n",
    "    # Loop over increasing PCA dimensions.\n",
    "    for n in range(1, max_components + 1):\n",
    "        # Create a pipeline with n_components in the PCA step.\n",
    "        model_pipe = Pipeline([\n",
    "            (\"scale\", StandardScaler()),\n",
    "            (\"pca\", PCA(n_components=n)),\n",
    "            (\"mod\", model)\n",
    "        ])\n",
    "        \n",
    "        # Run DFS-based feature selection with the current pipeline.\n",
    "        selected_features = Feature_selection(S, X, y, model_pipe)\n",
    "        # Compute average MR2 using leave-one-county-out CV.\n",
    "        avg_mr2 = compute_avg_MR2(selected_features, X, y, model_pipe)\n",
    "        print(f\"n_components = {n}, selected features = {selected_features}, MR2 = {avg_mr2}\")\n",
    "        \n",
    "        # If MR2 improves, update best values; if it drops, stop and return the previous best.\n",
    "        if avg_mr2 > best_mr2:\n",
    "            best_mr2 = avg_mr2\n",
    "            best_n = n\n",
    "            best_features = selected_features\n",
    "        else:\n",
    "            # MR2 has decreased; we assume the previous configuration was optimal.\n",
    "            break\n",
    "            \n",
    "    return best_n, best_features#, best_mr2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(fitted_models,features,size):\n",
    "    coeffs_mlr = fitted_models[\"mlr\"].named_steps[\"Lin\"].coef_\n",
    "    sorted_indices_mlr = np.argsort(coeffs_mlr)\n",
    "    plt.figure(figsize=size)\n",
    "    plt.barh([features[i] for i in sorted_indices_mlr], coeffs_mlr[sorted_indices_mlr])\n",
    "    plt.title(\"Linear Regression Coefficient Infulence\")\n",
    "    plt.xlabel(\"Coefficients\")\n",
    "    plt.show()\n",
    "\n",
    "    ll=[(abs(coeffs_mlr[i]),features[i]) for i in sorted_indices_mlr]\n",
    "    ll.sort(reverse=True)\n",
    "    print('The top 4 important features in mlr is')\n",
    "    for xx in ll[:4]:\n",
    "        print(xx[1])\n",
    "\n",
    "    coeffs_ridge = fitted_models[\"ridge\"].named_steps['ridge'].coef_\n",
    "    sorted_indices_ridge = np.argsort(coeffs_ridge)\n",
    "    plt.figure(figsize=size)\n",
    "    plt.barh([features[i] for i in sorted_indices_ridge], coeffs_ridge[sorted_indices_ridge])\n",
    "    plt.title(\"Ridge Coefficient Infulence\")\n",
    "    plt.xlabel(\"Coefficients\")\n",
    "    plt.show()\n",
    "\n",
    "    ll=[(abs(coeffs_ridge[i]),features[i]) for i in sorted_indices_ridge]\n",
    "    ll.sort(reverse=True)\n",
    "    print('The top 4 important features in ridge is')\n",
    "    for xx in ll[:4]:\n",
    "        print(xx[1])\n",
    "\n",
    "    importance_xgb = fitted_models[\"xgb\"].feature_importances_\n",
    "    sorted_indices_xgb = np.argsort(importance_xgb)\n",
    "    plt.figure(figsize=size)\n",
    "    plt.barh([features[i] for i in sorted_indices_xgb], importance_xgb[sorted_indices_xgb])\n",
    "    plt.title(\"XGB Coefficient Infulence\")\n",
    "    plt.xlabel(\"Coefficients\")\n",
    "    plt.show()\n",
    "\n",
    "    ll=[(abs(importance_xgb[i]),features[i]) for i in sorted_indices_xgb]\n",
    "    ll.sort(reverse=True)\n",
    "    print('The top 4 important features in xgb is')\n",
    "    for xx in ll[:4]:\n",
    "        print(xx[1])\n",
    "\n",
    "\n",
    "    importance_randomforest = fitted_models[\"random_forest\"].named_steps[\"randomforest\"].feature_importances_\n",
    "    sorted_indices_randomforest = np.argsort(importance_randomforest)\n",
    "    plt.figure(figsize=size)\n",
    "    plt.barh([features[i] for i in sorted_indices_randomforest], importance_randomforest[sorted_indices_randomforest])\n",
    "    plt.title(\"RandomForest Coefficient Infulence\")\n",
    "    plt.xlabel(\"Coefficients\")\n",
    "    plt.show()\n",
    "\n",
    "    ll=[(abs(importance_randomforest[i]),features[i]) for i in sorted_indices_randomforest]\n",
    "    ll.sort(reverse=True)\n",
    "    print('The top 4 important features in randomforest is')\n",
    "    for xx in ll[:4]:\n",
    "        print(xx[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_models={}\n",
    "models_stats={}\n",
    "models_pred={}\n",
    "\n",
    "# Uncomment this to perform feature selection again\n",
    "#features={}\n",
    "\n",
    "#Load selected features:\n",
    "features = joblib.load(\"features.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Urban Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imputed Urban model\n",
      "Optimal subset of features:\n",
      "13 ['uninsured_rate^2', 'clearance_rate home_ownership_rate', 'home_ownership_rate', 'Population vacancy_rate', 'adjusted_income renter_ratio', 'adj_welfare_budget house_affordability', 'rent_burden median_age', 'unemployment_rate uninsured_rate', 'adj_prison_budget', 'clearance_rate public_school_rate', 'security_vs_social', 'unemployment_rate median_house_value', 'Population adj_rehab_budget']\n",
      "╔═══════╦══════════════╦════════════════════════╦═══════════════════════╦══════════════════════╦════════════════════╗\n",
      "║ Model ║     Type     ║          MSE           ║          RMSE         ║       r2 Score       ║        MR2         ║\n",
      "╠═══════╬══════════════╬════════════════════════╬═══════════════════════╬══════════════════════╬════════════════════╣\n",
      "║ ridge ║  Reg train   ║ 2.651116551922819e-06  ║ 0.0016267122211047695 ║  0.6976419989748679  ║ 0.6976419989748679 ║\n",
      "║       ║   Reg val    ║ 2.7276884859081606e-06 ║  0.001633079666280068 ║  0.6930126621128654  ║ 0.6937825474539668 ║\n",
      "║       ║ County train ║ 2.575394992679449e-06  ║ 0.0016032472911000873 ║  0.7048216940470949  ║ 0.7048216940470949 ║\n",
      "║       ║  County val  ║ 3.3555137674716148e-06 ║ 0.0016496019649077415 ║ -0.02590283948357495 ║ 0.6709310107651281 ║\n",
      "║       ║  Time train  ║ 2.6573705383471842e-06 ║ 0.0016301443305263446 ║  0.7085281376055985  ║ 0.7085281376055985 ║\n",
      "║       ║   Time val   ║ 1.8816258769444396e-06 ║ 0.0013717236882639446 ║  0.5060368912906078  ║ 0.7237922989625489 ║\n",
      "║       ║     Test     ║  2.11174452585442e-06  ║ 0.0014531842711282077 ║  0.7022046807594864  ║ 0.7022086956381992 ║\n",
      "║       ║              ║                        ║                       ║                      ║                    ║\n",
      "╚═══════╩══════════════╩════════════════════════╩═══════════════════════╩══════════════════════╩════════════════════╝\n",
      "The top 4 important features in ridge are:\n",
      "uninsured_rate^2\n",
      "home_ownership_rate\n",
      "clearance_rate home_ownership_rate\n",
      "security_vs_social\n"
     ]
    }
   ],
   "source": [
    "key=\"imputed\"\n",
    "key1=\"Urban\"\n",
    "print(key+\" \"+key1+\" model\")\n",
    "\n",
    "n_components = 5\n",
    "alpha=100\n",
    "models = {\n",
    "    #\"mlr\": Pipeline([(\"scale\", StandardScaler()),('pca',PCA(n_components=n_components)), (\"Lin\", LinearRegression())]),\n",
    "    #\"xgb\": xgb.XGBRegressor(random_state=42,max_depth=3, n_estimators=100),\n",
    "    \"ridge\": Pipeline([(\"scale\", StandardScaler()),('pca',PCA(n_components=n_components,whiten=True)), (\"ridge\", Ridge(alpha))]),\n",
    "    #\"random_forest\": Pipeline([(\"scale\", StandardScaler()),('pca',PCA(n_components=n_components)), (\"randomforest\", RandomForestRegressor())]),\n",
    "}\n",
    "\n",
    "threshold_addition=0.0005\n",
    "threshold_removal=0.002\n",
    "#Uncomment this to perform feature selection again\n",
    "#features[(key,key1)]=list(Poly_feature_selection(set(X_train_Poly_dt[(key,key1)].columns), X_train_Poly_dt[(key,key1)], y_train_dt[(key,key1)], models[\"ridge\"], threshold_addition=threshold_addition, threshold_removal=threshold_removal))\n",
    "\n",
    "ff=features[(key,key1)]\n",
    "print(\"Optimal subset of features:\")\n",
    "print(len(ff),ff)\n",
    "\n",
    "\n",
    "fitted_models[(key,key1)], models_stats[(key,key1)], models_pred[(key,key1)] = deep_cross_fit1(\n",
    "    X_train_Poly_dt[(key,key1)][ff],X_test_Poly_dt[(key,key1)][ff], y_train_dt[(key,key1)],y_test_dt[(key,key1)], **models\n",
    ")\n",
    "\n",
    "# Using pretty table\n",
    "print_table(models_stats[(key,key1)])\n",
    "feature_importance_PCA(fitted_models[(key,key1)],features[(key,key1)],(3,3), plot=False, models_to_use=['ridge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropna Urban model\n",
      "Optimal subset of features:\n",
      "15 ['adjusted_income uninsured_rate', 'adjusted_income^2', 'dropout_rate', 'adjusted_income', 'rent_burden adj_judiciary_budget', 'mobile_home_ratio high_school_rate', 'home_ownership_rate adj_police_budget', 'median_age', 'clearance_rate poverty_rate', 'adj_prison_budget', 'median_age adj_judiciary_budget', 'security_vs_social', 'median_age^2', 'clearance_rate', 'unemployment_rate adj_police_budget']\n",
      "╔═══════╦══════════════╦════════════════════════╦═══════════════════════╦═════════════════════╦════════════════════╗\n",
      "║ Model ║     Type     ║          MSE           ║          RMSE         ║       r2 Score      ║        MR2         ║\n",
      "╠═══════╬══════════════╬════════════════════════╬═══════════════════════╬═════════════════════╬════════════════════╣\n",
      "║ ridge ║  Reg train   ║ 7.020646040085065e-07  ║ 0.0008374428616650357 ║  0.8092759078304479 ║ 0.8092759078304479 ║\n",
      "║       ║   Reg val    ║ 7.849230281538913e-07  ║ 0.0008661888045140403 ║  0.785338594876212  ║ 0.7994874319600869 ║\n",
      "║       ║ County train ║ 6.775822633213537e-07  ║  0.000821719142023402 ║  0.8147392447435137 ║ 0.8147392447435137 ║\n",
      "║       ║  County val  ║ 9.301068301176784e-07  ║  0.000779746240212533 ║ -15.275101699389129 ║ 0.7290764823118494 ║\n",
      "║       ║  Time train  ║ 6.040523762629619e-07  ║ 0.0007772080649755006 ║  0.8150936411854932 ║ 0.8150936411854932 ║\n",
      "║       ║   Time val   ║ 1.3274546162530571e-06 ║ 0.0011521521671433235 ║  0.7115741538538674 ║ 0.7261220010831222 ║\n",
      "║       ║     Test     ║ 7.403439550255455e-07  ║ 0.0008604324232765439 ║  0.7581857768507503 ║ 0.7663287499743975 ║\n",
      "║       ║              ║                        ║                       ║                     ║                    ║\n",
      "╚═══════╩══════════════╩════════════════════════╩═══════════════════════╩═════════════════════╩════════════════════╝\n",
      "The top 4 important features in ridge are:\n",
      "dropout_rate\n",
      "clearance_rate\n",
      "security_vs_social\n",
      "home_ownership_rate adj_police_budget\n"
     ]
    }
   ],
   "source": [
    "key=\"dropna\"\n",
    "key1=\"Urban\"\n",
    "print(key+\" \"+key1+\" model\")\n",
    "\n",
    "n_components = 5\n",
    "alpha=30\n",
    "models = {\n",
    "    #\"mlr\": Pipeline([(\"scale\", StandardScaler()),('pca',PCA(n_components=n_components)), (\"Lin\", LinearRegression())]),\n",
    "    #\"xgb\": xgb.XGBRegressor(random_state=42,max_depth=3, n_estimators=100),\n",
    "    \"ridge\": Pipeline([(\"scale\", StandardScaler()),('pca',PCA(n_components=n_components,whiten=True)), (\"ridge\", Ridge(alpha=alpha))]),\n",
    "    #\"random_forest\": Pipeline([(\"scale\", StandardScaler()),('pca',PCA(n_components=n_components)), (\"randomforest\", RandomForestRegressor())]),\n",
    "}\n",
    "\n",
    "threshold_addition=0.0005\n",
    "threshold_removal=0.002\n",
    "#Uncomment this to perform feature selection again\n",
    "#features[(key,key1)]=list(Poly_feature_selection(set(X_train_Poly_dt[(key,key1)].columns), X_train_Poly_dt[(key,key1)], y_train_dt[(key,key1)], models[\"ridge\"], threshold_addition=threshold_addition, threshold_removal=threshold_removal,dd=35))\n",
    "\n",
    "ff=features[(key,key1)]\n",
    "print(\"Optimal subset of features:\")\n",
    "print(len(ff),ff)\n",
    "\n",
    "\n",
    "fitted_models[(key,key1)], models_stats[(key,key1)], models_pred[(key,key1)] = deep_cross_fit1(\n",
    "    X_train_Poly_dt[(key,key1)][ff],X_test_Poly_dt[(key,key1)][ff], y_train_dt[(key,key1)],y_test_dt[(key,key1)], **models\n",
    ")\n",
    "\n",
    "# Using pretty table\n",
    "print_table(models_stats[(key,key1)])\n",
    "feature_importance_PCA(fitted_models[(key,key1)],features[(key,key1)],(3,3), plot=False, models_to_use=['ridge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suburban models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imputed Suburban model\n",
      "Optimal subset of features:\n",
      "18 ['unemployment_rate mobile_home_ratio', 'Population', 'mobile_home_ratio adj_welfare_budget', 'adj_education_budget social_vs_security', 'uninsured_rate', 'adjusted_income high_school_rate', 'rent_burden mobile_home_ratio', 'median_house_value', 'public_school_rate', 'adjusted_income^2', 'home_ownership_rate median_house_value', 'mobile_home_ratio', 'poverty_rate adj_police_budget', 'clearance_rate Number_of_Persons_per_HseHld', 'poverty_rate median_age', 'adjusted_income median_house_value', 'clearance_rate', 'population_density']\n",
      "╔═══════╦══════════════╦════════════════════════╦═══════════════════════╦══════════════════════╦═════════════════════╗\n",
      "║ Model ║     Type     ║          MSE           ║          RMSE         ║       r2 Score       ║         MR2         ║\n",
      "╠═══════╬══════════════╬════════════════════════╬═══════════════════════╬══════════════════════╬═════════════════════╣\n",
      "║ ridge ║  Reg train   ║ 1.5950361766145837e-06 ║  0.001262691999487455 ║ 0.49847480581490283  ║  0.4984748058149028 ║\n",
      "║       ║   Reg val    ║ 1.642425673802448e-06  ║ 0.0012763669006410896 ║ 0.48348130022217556  ║ 0.48702739857744926 ║\n",
      "║       ║ County train ║ 1.592353592394988e-06  ║ 0.0012614863772549226 ║  0.4979971023018957  ║  0.4979971023018957 ║\n",
      "║       ║  County val  ║ 1.8678542638363135e-06 ║ 0.0012954037654193634 ║ -0.30324836810410893 ║ 0.35463721323007735 ║\n",
      "║       ║  Time train  ║ 1.6717499941527545e-06 ║ 0.0012929617141094142 ║  0.4830548339693336  ║  0.4830548339693337 ║\n",
      "║       ║   Time val   ║ 1.719858246598131e-06  ║ 0.0013114336607690574 ║ 0.15394083706712502  ║ 0.42129935262774887 ║\n",
      "║       ║     Test     ║ 1.7618765596669607e-06 ║ 0.0013273569827544362 ║  0.4092203693592559  ║  0.4117529470254899 ║\n",
      "║       ║              ║                        ║                       ║                      ║                     ║\n",
      "╚═══════╩══════════════╩════════════════════════╩═══════════════════════╩══════════════════════╩═════════════════════╝\n"
     ]
    }
   ],
   "source": [
    "key=\"imputed\"\n",
    "key1=\"Suburban\"\n",
    "print(key+\" \"+key1+\" model\")\n",
    "\n",
    "n_components = 6\n",
    "alpha=100\n",
    "models = {\n",
    "    #\"mlr\": Pipeline([(\"scale\", StandardScaler()),('pca',PCA(n_components=n_components)), (\"Lin\", LinearRegression())]),\n",
    "    #\"xgb\": xgb.XGBRegressor(random_state=42,max_depth=3, n_estimators=100),\n",
    "    \"ridge\": Pipeline([(\"scale\", StandardScaler()),('pca',PCA(n_components=n_components,whiten=True)), (\"ridge\", Ridge(alpha=alpha))]),\n",
    "    #\"random_forest\": Pipeline([(\"scale\", StandardScaler()),('pca',PCA(n_components=n_components)), (\"randomforest\", RandomForestRegressor())]),\n",
    "}\n",
    "\n",
    "threshold_addition=0.0005\n",
    "threshold_removal=0.002\n",
    "#Uncomment this to perform feature selection again\n",
    "#features[(key,key1)]=list(Poly_feature_selection(set(X_train_Poly_dt[(key,key1)].columns), X_train_Poly_dt[(key,key1)], y_train_dt[(key,key1)], models[\"ridge\"], threshold_addition=threshold_addition, threshold_removal=threshold_removal,dd=35))\n",
    "\n",
    "ff=features[(key,key1)]\n",
    "print(\"Optimal subset of features:\")\n",
    "print(len(ff),ff)\n",
    "\n",
    "\n",
    "fitted_models[(key,key1)], models_stats[(key,key1)], models_pred[(key,key1)] = deep_cross_fit1(\n",
    "    X_train_Poly_dt[(key,key1)][ff],X_test_Poly_dt[(key,key1)][ff], y_train_dt[(key,key1)],y_test_dt[(key,key1)], **models\n",
    ")\n",
    "\n",
    "# Using pretty table\n",
    "print_table(models_stats[(key,key1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropna Suburban model\n",
      "Processing block 0 to 35\n",
      "Processing block 35 to 70\n",
      "Processing block 70 to 105\n",
      "Processing block 105 to 140\n",
      "Processing block 140 to 175\n",
      "Processing block 175 to 210\n",
      "Processing block 210 to 245\n",
      "Processing block 245 to 280\n",
      "Processing block 280 to 315\n",
      "Processing block 315 to 350\n",
      "Processing block 350 to 385\n",
      "Processing block 385 to 420\n",
      "Processing block 420 to 455\n",
      "Processing block 455 to 490\n",
      "Optimal subset of features:\n",
      "13 ['median_age adj_welfare_budget', 'adj_mental_health_budget public_school_rate', 'Population adj_mental_health_budget', 'mobile_home_ratio security_vs_social', 'Population^2', 'adj_police_budget dropout_rate', 'adjusted_income mobile_home_ratio', 'Population median_house_value', 'vacancy_rate adj_rehab_budget', 'clearance_rate adjusted_income', 'Number_of_Persons_per_HseHld adj_health_budget', 'vacancy_rate', 'mobile_home_ratio median_age']\n",
      "╔═══════╦══════════════╦════════════════════════╦═══════════════════════╦═════════════════════╦═════════════════════╗\n",
      "║ Model ║     Type     ║          MSE           ║          RMSE         ║       r2 Score      ║         MR2         ║\n",
      "╠═══════╬══════════════╬════════════════════════╬═══════════════════════╬═════════════════════╬═════════════════════╣\n",
      "║ ridge ║  Reg train   ║ 1.084827520840657e-06  ║ 0.0010407826494649483 ║  0.4254611480503951 ║  0.4254611480503951 ║\n",
      "║       ║   Reg val    ║ 1.118499863437804e-06  ║ 0.0010469014739057322 ║ 0.40966936802221865 ║  0.4189537803497417 ║\n",
      "║       ║ County train ║ 1.0513864176499912e-06 ║  0.001024199507634745 ║ 0.44139183764083195 ║  0.4413918376408319 ║\n",
      "║       ║  County val  ║ 1.2168130185264114e-06 ║ 0.0009534549426164902 ║  -4.634447392434016 ║  0.1765425216677351 ║\n",
      "║       ║  Time train  ║  9.71473617560713e-07  ║ 0.0009856336122316006 ║ 0.45920895898709213 ║ 0.45920895898709213 ║\n",
      "║       ║   Time val   ║ 1.4305999863957478e-06 ║ 0.0011960769149163225 ║  0.3054422535583967 ║  0.3182430851745971 ║\n",
      "║       ║     Test     ║ 1.119098078236703e-06  ║ 0.0010578743206244789 ║ 0.23524573533903037 ║ 0.23538683748699119 ║\n",
      "║       ║              ║                        ║                       ║                     ║                     ║\n",
      "╚═══════╩══════════════╩════════════════════════╩═══════════════════════╩═════════════════════╩═════════════════════╝\n"
     ]
    }
   ],
   "source": [
    "key=\"dropna\"\n",
    "key1=\"Suburban\"\n",
    "print(key+\" \"+key1+\" model\")\n",
    "\n",
    "n_components = 6\n",
    "alpha=100\n",
    "models = {\n",
    "    #\"mlr\": Pipeline([(\"scale\", StandardScaler()),('pca',PCA(n_components=n_components)), (\"Lin\", LinearRegression())]),\n",
    "    #\"xgb\": xgb.XGBRegressor(random_state=42,max_depth=3, n_estimators=100),\n",
    "    \"ridge\": Pipeline([(\"scale\", StandardScaler()),('pca',PCA(n_components=n_components,whiten=True)), (\"ridge\", Ridge(alpha=alpha))]),\n",
    "    #\"random_forest\": Pipeline([(\"scale\", StandardScaler()),('pca',PCA(n_components=n_components)), (\"randomforest\", RandomForestRegressor())]),\n",
    "}\n",
    "\n",
    "threshold_addition=0.0005\n",
    "threshold_removal=0.002\n",
    "#Uncomment this to perform feature selection again\n",
    "features[(key,key1)]=list(Poly_feature_selection(set(X_train_Poly_dt[(key,key1)].columns), X_train_Poly_dt[(key,key1)], y_train_dt[(key,key1)], models[\"ridge\"], threshold_addition=threshold_addition, threshold_removal=threshold_removal,dd=35))\n",
    "\n",
    "ff=features[(key,key1)]\n",
    "print(\"Optimal subset of features:\")\n",
    "print(len(ff),ff)\n",
    "\n",
    "\n",
    "fitted_models[(key,key1)], models_stats[(key,key1)], models_pred[(key,key1)] = deep_cross_fit1(\n",
    "    X_train_Poly_dt[(key,key1)][ff],X_test_Poly_dt[(key,key1)][ff], y_train_dt[(key,key1)],y_test_dt[(key,key1)], **models\n",
    ")\n",
    "\n",
    "# Using pretty table\n",
    "print_table(models_stats[(key,key1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rural models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imputed Rural model\n",
      "Optimal subset of features:\n",
      "16 ['adjusted_income public_school_rate', 'vacancy_rate', 'adj_rehab_budget', 'rent_burden social_vs_security', 'social_vs_security security_vs_social', 'adj_judiciary_budget public_school_rate', 'Population mobile_home_ratio', 'adj_education_budget^2', 'rent_burden uninsured_rate', 'adj_rehab_budget adj_prison_budget', 'rent_burden', 'population_density home_ownership_rate', 'population_density rent_burden', 'Number_of_Persons_per_HseHld', 'unemployment_rate median_house_value', 'poverty_rate']\n",
      "╔═══════╦══════════════╦════════════════════════╦═══════════════════════╦══════════════════════╦══════════════════════╗\n",
      "║ Model ║     Type     ║          MSE           ║          RMSE         ║       r2 Score       ║         MR2          ║\n",
      "╠═══════╬══════════════╬════════════════════════╬═══════════════════════╬══════════════════════╬══════════════════════╣\n",
      "║ ridge ║  Reg train   ║  4.81996363546391e-06  ║  0.002193668851644663 ║  0.2611994289221312  ║  0.2611994289221312  ║\n",
      "║       ║   Reg val    ║ 5.224222379886138e-06  ║ 0.0022465421517318665 ║  0.1825907981115706  ║ 0.18833567588431982  ║\n",
      "║       ║ County train ║ 4.848706430401969e-06  ║ 0.0022011930544482746 ║  0.2600170405937869  ║  0.2600170405937869  ║\n",
      "║       ║  County val  ║ 1.6479522175148504e-05 ║ 0.0024396207555533703 ║ -0.7766516834652393  ║ -0.07779208611380845 ║\n",
      "║       ║  Time train  ║ 5.037044550837244e-06  ║  0.002244336104694937 ║  0.2723865006632088  ║  0.2723865006632088  ║\n",
      "║       ║   Time val   ║ 3.779309299167595e-06  ║ 0.0019440445723201913 ║ 0.059588163734013766 ║ 0.05964974795092637  ║\n",
      "║       ║     Test     ║ 4.927641703800084e-06  ║ 0.0022198292059976334 ║ 0.12792756978472564  ║  0.1280896799085166  ║\n",
      "║       ║              ║                        ║                       ║                      ║                      ║\n",
      "╚═══════╩══════════════╩════════════════════════╩═══════════════════════╩══════════════════════╩══════════════════════╝\n"
     ]
    }
   ],
   "source": [
    "key=\"imputed\"\n",
    "key1=\"Rural\"\n",
    "print(key+\" \"+key1+\" model\")\n",
    "\n",
    "n_components = 5\n",
    "alpha=100\n",
    "models = {\n",
    "    #\"mlr\": Pipeline([(\"scale\", StandardScaler()),('pca',PCA(n_components=n_components)), (\"Lin\", LinearRegression())]),\n",
    "    #\"xgb\": xgb.XGBRegressor(random_state=42,max_depth=3, n_estimators=100),\n",
    "    \"ridge\": Pipeline([(\"scale\", StandardScaler()),('pca',PCA(n_components=n_components,whiten=True)), (\"ridge\", Ridge(alpha=alpha))]),\n",
    "    #\"random_forest\": Pipeline([(\"scale\", StandardScaler()),('pca',PCA(n_components=n_components)), (\"randomforest\", RandomForestRegressor())]),\n",
    "}\n",
    "\n",
    "threshold_addition=0.0005\n",
    "threshold_removal=0.002\n",
    "#Uncomment this to perform feature selection again\n",
    "#features[(key,key1)]=list(Poly_feature_selection(set(X_train_Poly_dt[(key,key1)].columns), X_train_Poly_dt[(key,key1)], y_train_dt[(key,key1)], models[\"ridge\"], threshold_addition=threshold_addition, threshold_removal=threshold_removal,dd=35))\n",
    "\n",
    "ff=features[(key,key1)]\n",
    "print(\"Optimal subset of features:\")\n",
    "print(len(ff),ff)\n",
    "\n",
    "\n",
    "fitted_models[(key,key1)], models_stats[(key,key1)], models_pred[(key,key1)] = deep_cross_fit1(\n",
    "    X_train_Poly_dt[(key,key1)][ff],X_test_Poly_dt[(key,key1)][ff], y_train_dt[(key,key1)],y_test_dt[(key,key1)], **models\n",
    ")\n",
    "\n",
    "# Using pretty table\n",
    "print_table(models_stats[(key,key1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropna Rural model\n",
      "Optimal subset of features:\n",
      "21 ['Number_of_Persons_per_HseHld^2', 'clearance_rate house_affordability', 'renter_ratio^2', 'adjusted_income uninsured_rate', 'uninsured_rate security_vs_social', 'adjusted_income poverty_rate', 'uninsured_rate', 'mobile_home_ratio adj_prison_budget', 'mobile_home_ratio median_house_value', 'adj_judiciary_budget median_house_value', 'home_ownership_rate adj_judiciary_budget', 'adj_judiciary_budget public_school_rate', 'Population mobile_home_ratio', 'adj_education_budget^2', 'median_house_value', 'median_house_value security_vs_social', 'adjusted_income median_age', 'poverty_rate Number_of_Persons_per_HseHld', 'dropout_rate', 'adjusted_income mobile_home_ratio', 'Number_of_Persons_per_HseHld']\n",
      "╔═══════╦══════════════╦════════════════════════╦═══════════════════════╦═════════════════════╦═════════════════════╗\n",
      "║ Model ║     Type     ║          MSE           ║          RMSE         ║       r2 Score      ║         MR2         ║\n",
      "╠═══════╬══════════════╬════════════════════════╬═══════════════════════╬═════════════════════╬═════════════════════╣\n",
      "║ ridge ║  Reg train   ║ 1.7340051536174468e-06 ║ 0.0013155271267657974 ║  0.5174897723371974 ║  0.5174897723371974 ║\n",
      "║       ║   Reg val    ║ 1.913771181320401e-06  ║  0.001355259076376184 ║ 0.41243210705670996 ║  0.4706264771464994 ║\n",
      "║       ║ County train ║ 1.730320255873295e-06  ║ 0.0013150664661806734 ║  0.5187091885083269 ║  0.5187091885083269 ║\n",
      "║       ║  County val  ║ 2.0546966868824517e-06 ║  0.001277480666970049 ║ -1.1965082176067843 ║  0.3038188698632824 ║\n",
      "║       ║  Time train  ║ 1.4378985274782743e-06 ║ 0.0011991240667580125 ║  0.4818301171041268 ║  0.4818301171041268 ║\n",
      "║       ║   Time val   ║  3.04081693707832e-06  ║ 0.0017437938344535802 ║ 0.37637376050873006 ║  0.4282829838060229 ║\n",
      "║       ║     Test     ║ 1.5479601544559398e-06 ║ 0.0012441704684069382 ║  0.4313277597396613 ║ 0.43167224390096937 ║\n",
      "║       ║              ║                        ║                       ║                     ║                     ║\n",
      "╚═══════╩══════════════╩════════════════════════╩═══════════════════════╩═════════════════════╩═════════════════════╝\n"
     ]
    }
   ],
   "source": [
    "key=\"dropna\"\n",
    "key1=\"Rural\"\n",
    "print(key+\" \"+key1+\" model\")\n",
    "\n",
    "n_components = 5\n",
    "alpha=100\n",
    "models = {\n",
    "    #\"mlr\": Pipeline([(\"scale\", StandardScaler()),('pca',PCA(n_components=n_components)), (\"Lin\", LinearRegression())]),\n",
    "    #\"xgb\": xgb.XGBRegressor(random_state=42,max_depth=3, n_estimators=100),\n",
    "    \"ridge\": Pipeline([(\"scale\", StandardScaler()),('pca',PCA(n_components=n_components)), (\"ridge\", Ridge(alpha=alpha))]),\n",
    "    #\"random_forest\": Pipeline([(\"scale\", StandardScaler()),('pca',PCA(n_components=n_components)), (\"randomforest\", RandomForestRegressor())]),\n",
    "}\n",
    "\n",
    "threshold_addition=0.0005\n",
    "threshold_removal=0.002\n",
    "#Uncomment this to perform feature selection again\n",
    "#features[(key,key1)]=list(Poly_feature_selection(set(X_train_Poly_dt[(key,key1)].columns), X_train_Poly_dt[(key,key1)], y_train_dt[(key,key1)], models[\"ridge\"], threshold_addition=threshold_addition, threshold_removal=threshold_removal))\n",
    "\n",
    "ff=features[(key,key1)]\n",
    "print(\"Optimal subset of features:\")\n",
    "print(len(ff),ff)\n",
    "\n",
    "\n",
    "fitted_models[(key,key1)], models_stats[(key,key1)], models_pred[(key,key1)] = deep_cross_fit1(\n",
    "    X_train_Poly_dt[(key,key1)][ff],X_test_Poly_dt[(key,key1)][ff], y_train_dt[(key,key1)],y_test_dt[(key,key1)], **models\n",
    ")\n",
    "\n",
    "# Using pretty table\n",
    "print_table(models_stats[(key,key1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['features.pkl']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To save the dictionary\n",
    "joblib.dump(features, \"features.pkl\")\n",
    "\n",
    "# To load it later\n",
    "#features = joblib.load(\"features.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "imputed Urban model\n",
      "The top 4 important features in ridge are:\n",
      "uninsured_rate^2\n",
      "home_ownership_rate\n",
      "clearance_rate home_ownership_rate\n",
      "security_vs_social\n",
      "\n",
      "\n",
      "dropna Urban model\n",
      "The top 4 important features in ridge are:\n",
      "dropout_rate\n",
      "clearance_rate\n",
      "security_vs_social\n",
      "home_ownership_rate adj_police_budget\n",
      "\n",
      "\n",
      "imputed Suburban model\n",
      "The top 4 important features in ridge are:\n",
      "public_school_rate\n",
      "uninsured_rate\n",
      "mobile_home_ratio\n",
      "adj_education_budget social_vs_security\n",
      "\n",
      "\n",
      "dropna Suburban model\n",
      "The top 4 important features in ridge are:\n",
      "adj_rehab_budget adj_judiciary_budget\n",
      "renter_ratio security_vs_social\n",
      "security_vs_social\n",
      "security_vs_social^2\n",
      "\n",
      "\n",
      "imputed Rural model\n",
      "The top 4 important features in ridge are:\n",
      "social_vs_security security_vs_social\n",
      "adj_rehab_budget\n",
      "vacancy_rate\n",
      "adj_education_budget^2\n",
      "\n",
      "\n",
      "dropna Rural model\n",
      "The top 4 important features in ridge are:\n",
      "uninsured_rate security_vs_social\n",
      "uninsured_rate\n",
      "mobile_home_ratio adj_prison_budget\n",
      "home_ownership_rate adj_judiciary_budget\n"
     ]
    }
   ],
   "source": [
    "for key1 in [\"Urban\",\"Suburban\",\"Rural\"]:\n",
    "    for key in [\"imputed\",\"dropna\"]:\n",
    "        print('\\n')\n",
    "        print(key+\" \"+key1+\" model\")\n",
    "        # Can set plot=True to visualize the feature importances\n",
    "        # Can modify models_to_use to include/exclude specific models\n",
    "        feature_importance_PCA(fitted_models[(key,key1)],features[(key,key1)],(3,3), plot=False, models_to_use=['ridge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt to find better hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imputed Urban model\n",
      "n_components 5 alpha 80\n",
      "Processing block 0 to 35\n",
      "Processing block 35 to 70\n",
      "Processing block 70 to 105\n",
      "Processing block 105 to 140\n",
      "Processing block 140 to 175\n",
      "Processing block 175 to 210\n",
      "Processing block 210 to 245\n",
      "Processing block 245 to 280\n",
      "Processing block 280 to 315\n",
      "Processing block 315 to 350\n",
      "Processing block 350 to 385\n",
      "Processing block 385 to 420\n",
      "Processing block 420 to 455\n",
      "Processing block 455 to 490\n",
      "Optimal subset of features:\n",
      "17 ['uninsured_rate^2', 'adjusted_income public_school_rate', 'adj_mental_health_budget^2', 'adjusted_income renter_ratio', 'unemployment_rate uninsured_rate', 'mobile_home_ratio median_house_value', 'adj_police_budget adj_education_budget', 'vacancy_rate house_affordability', 'clearance_rate median_house_value', 'median_house_value social_vs_security', 'clearance_rate home_ownership_rate', 'clearance_rate', 'adj_judiciary_budget adj_prison_budget', 'adj_welfare_budget house_affordability', 'clearance_rate adj_rehab_budget', 'poverty_rate adj_police_budget', 'high_school_rate security_vs_social']\n",
      "0.7065448449888763\n",
      "\n",
      "\n",
      "n_components 5 alpha 100\n",
      "Processing block 0 to 35\n",
      "Processing block 35 to 70\n",
      "Processing block 70 to 105\n",
      "Processing block 105 to 140\n",
      "Processing block 140 to 175\n",
      "Processing block 175 to 210\n",
      "Processing block 210 to 245\n",
      "Processing block 245 to 280\n",
      "Processing block 280 to 315\n",
      "Processing block 315 to 350\n",
      "Processing block 350 to 385\n",
      "Processing block 385 to 420\n",
      "Processing block 420 to 455\n",
      "Processing block 455 to 490\n",
      "Optimal subset of features:\n",
      "16 ['vacancy_rate', 'Population uninsured_rate', 'unemployment_rate uninsured_rate', 'social_vs_security', 'home_ownership_rate vacancy_rate', 'public_school_rate security_vs_social', 'Population adj_health_budget', 'median_house_value', 'clearance_rate^2', 'home_ownership_rate', 'home_ownership_rate social_vs_security', 'vacancy_rate security_vs_social', 'Population dropout_rate', 'adjusted_income mobile_home_ratio', 'clearance_rate vacancy_rate', 'clearance_rate public_school_rate']\n",
      "0.7122257028491215\n",
      "\n",
      "\n",
      "n_components 5 alpha 120\n",
      "Processing block 0 to 35\n",
      "Processing block 35 to 70\n",
      "Processing block 70 to 105\n",
      "Processing block 105 to 140\n",
      "Processing block 140 to 175\n",
      "Processing block 175 to 210\n",
      "Processing block 210 to 245\n",
      "Processing block 245 to 280\n",
      "Processing block 280 to 315\n",
      "Processing block 315 to 350\n",
      "Processing block 350 to 385\n",
      "Processing block 385 to 420\n",
      "Processing block 420 to 455\n",
      "Processing block 455 to 490\n",
      "Optimal subset of features:\n",
      "14 ['uninsured_rate^2', 'median_house_value', 'clearance_rate home_ownership_rate', 'home_ownership_rate', 'vacancy_rate security_vs_social', 'Population dropout_rate', 'Population uninsured_rate', 'adjusted_income mobile_home_ratio', 'adj_welfare_budget house_affordability', 'unemployment_rate uninsured_rate', 'home_ownership_rate vacancy_rate', 'clearance_rate renter_ratio', 'security_vs_social', 'Population median_house_value']\n",
      "0.6917258299123137\n",
      "\n",
      "\n",
      "n_components 6 alpha 80\n",
      "Processing block 0 to 35\n",
      "Processing block 35 to 70\n",
      "Processing block 70 to 105\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "n_components=6 must be between 0 and min(n_samples, n_features)=5 with svd_solver='covariance_eigh'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m threshold_removal\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.002\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#Uncomment this to perform feature selection again\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m features[(key,key1)]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[43mPoly_feature_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_train_Poly_dt\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkey1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_Poly_dt\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkey1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_dt\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkey1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold_addition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold_addition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold_removal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold_removal\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m35\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     17\u001b[0m ff\u001b[38;5;241m=\u001b[39mfeatures[(key,key1)]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimal subset of features:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[46], line 49\u001b[0m, in \u001b[0;36mPoly_feature_selection\u001b[1;34m(S, X, y, model, threshold_addition, threshold_removal, dd)\u001b[0m\n\u001b[0;32m     47\u001b[0m     S0 \u001b[38;5;241m=\u001b[39m add_features(S0, block, X, y, model, threshold\u001b[38;5;241m=\u001b[39mthreshold_addition)\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# Then run removal on the new S0 to prune any redundant features.\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m     S0 \u001b[38;5;241m=\u001b[39m \u001b[43mremove_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mS0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold_removal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# After processing all blocks, ensure that the base features remain if they can be beneficial:\u001b[39;00m\n\u001b[0;32m     52\u001b[0m S0 \u001b[38;5;241m=\u001b[39m add_features(S0, \u001b[38;5;28mset\u001b[39m(feature_0), X, y, model, threshold\u001b[38;5;241m=\u001b[39mthreshold_addition)\n",
      "Cell \u001b[1;32mIn[16], line 166\u001b[0m, in \u001b[0;36mremove_features\u001b[1;34m(S, X, y, model, threshold)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# Accept removal if the drop in average MR2 is less than or equal to threshold.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_avg \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m current_avg \u001b[38;5;241m-\u001b[39m threshold:\n\u001b[1;32m--> 166\u001b[0m     candidate \u001b[38;5;241m=\u001b[39m \u001b[43mremove_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m     candidate_avg \u001b[38;5;241m=\u001b[39m compute_avg_MR2_fast(candidate, X, y, model)\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m candidate_avg \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m current_avg \u001b[38;5;241m-\u001b[39m threshold:\n",
      "Cell \u001b[1;32mIn[16], line 166\u001b[0m, in \u001b[0;36mremove_features\u001b[1;34m(S, X, y, model, threshold)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# Accept removal if the drop in average MR2 is less than or equal to threshold.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_avg \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m current_avg \u001b[38;5;241m-\u001b[39m threshold:\n\u001b[1;32m--> 166\u001b[0m     candidate \u001b[38;5;241m=\u001b[39m \u001b[43mremove_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m     candidate_avg \u001b[38;5;241m=\u001b[39m compute_avg_MR2_fast(candidate, X, y, model)\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m candidate_avg \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m current_avg \u001b[38;5;241m-\u001b[39m threshold:\n",
      "    \u001b[1;31m[... skipping similar frames: remove_features at line 166 (11 times)]\u001b[0m\n",
      "Cell \u001b[1;32mIn[16], line 166\u001b[0m, in \u001b[0;36mremove_features\u001b[1;34m(S, X, y, model, threshold)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# Accept removal if the drop in average MR2 is less than or equal to threshold.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_avg \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m current_avg \u001b[38;5;241m-\u001b[39m threshold:\n\u001b[1;32m--> 166\u001b[0m     candidate \u001b[38;5;241m=\u001b[39m \u001b[43mremove_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m     candidate_avg \u001b[38;5;241m=\u001b[39m compute_avg_MR2_fast(candidate, X, y, model)\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m candidate_avg \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m current_avg \u001b[38;5;241m-\u001b[39m threshold:\n",
      "Cell \u001b[1;32mIn[16], line 163\u001b[0m, in \u001b[0;36mremove_features\u001b[1;34m(S, X, y, model, threshold)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m new_set:  \u001b[38;5;66;03m# skip if empty\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 163\u001b[0m new_avg \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_avg_MR2_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# Accept removal if the drop in average MR2 is less than or equal to threshold.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_avg \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m current_avg \u001b[38;5;241m-\u001b[39m threshold:\n",
      "Cell \u001b[1;32mIn[16], line 112\u001b[0m, in \u001b[0;36mcompute_avg_MR2_fast\u001b[1;34m(S, X, y, model, n_splits, random_state)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Clone and fit the model on the current training fold\u001b[39;00m\n\u001b[0;32m    111\u001b[0m model_clone \u001b[38;5;241m=\u001b[39m clone(model)\n\u001b[1;32m--> 112\u001b[0m \u001b[43mmodel_clone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model_clone\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[0;32m    114\u001b[0m mse_model \u001b[38;5;241m=\u001b[39m mean_squared_error(y_val, y_pred)\n",
      "File \u001b[1;32mc:\\Users\\deepe\\anaconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\deepe\\anaconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\sklearn\\pipeline.py:654\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    647\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    648\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `transform_input` parameter can only be set if metadata \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    649\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouting is enabled. You can enable metadata routing using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    650\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`sklearn.set_config(enable_metadata_routing=True)`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    651\u001b[0m     )\n\u001b[0;32m    653\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m--> 654\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\deepe\\anaconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\sklearn\\pipeline.py:588\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, routed_params, raw_params)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m    582\u001b[0m step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[0;32m    583\u001b[0m     step_idx\u001b[38;5;241m=\u001b[39mstep_idx,\n\u001b[0;32m    584\u001b[0m     step_params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[0;32m    585\u001b[0m     all_params\u001b[38;5;241m=\u001b[39mraw_params,\n\u001b[0;32m    586\u001b[0m )\n\u001b[1;32m--> 588\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32mc:\\Users\\deepe\\anaconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\joblib\\memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\deepe\\anaconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\sklearn\\pipeline.py:1551\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m   1550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1551\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1552\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1553\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   1554\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m   1555\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\deepe\\anaconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\deepe\\anaconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\deepe\\anaconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:468\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    447\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 468\u001b[0m     U, S, _, X, x_is_centered, xp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m U \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m         U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n",
      "File \u001b[1;32mc:\\Users\\deepe\\anaconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:542\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;66;03m# Call different fits for either full or truncated SVD\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcovariance_eigh\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_array_api_compliant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandomized\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_truncated(X, n_components, xp)\n",
      "File \u001b[1;32mc:\\Users\\deepe\\anaconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:556\u001b[0m, in \u001b[0;36mPCA._fit_full\u001b[1;34m(self, X, n_components, xp, is_array_api_compliant)\u001b[0m\n\u001b[0;32m    552\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmle\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is only supported if n_samples >= n_features\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    554\u001b[0m         )\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n_components \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_samples, n_features):\n\u001b[1;32m--> 556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    557\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be between 0 and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    558\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin(n_samples, n_features)=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(n_samples,\u001b[38;5;250m \u001b[39mn_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    559\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvd_solver=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    560\u001b[0m     )\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_ \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mmean(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    563\u001b[0m \u001b[38;5;66;03m# When X is a scipy sparse matrix, self.mean_ is a numpy matrix, so we need\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# to transform it to a 1D array. Note that this is not the case when X\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;66;03m# is a scipy sparse array.\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;66;03m# TODO: remove the following two lines when scikit-learn only depends\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;66;03m# on scipy versions that no longer support scipy.sparse matrices.\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: n_components=6 must be between 0 and min(n_samples, n_features)=5 with svd_solver='covariance_eigh'"
     ]
    }
   ],
   "source": [
    "key=\"imputed\"\n",
    "key1=\"Urban\"\n",
    "print(key+\" \"+key1+\" model\")\n",
    "\n",
    "\n",
    "for n_components in [5,6,7]:\n",
    "    for alpha in [80,100,120]:\n",
    "        print(\"n_components\",n_components,\"alpha\",alpha)\n",
    "\n",
    "        pipe=Pipeline([(\"scale\", StandardScaler()),('pca',PCA(n_components=n_components,whiten=True)), (\"ridge\", Ridge(alpha))])\n",
    "        \n",
    "        threshold_addition=0.0005\n",
    "        threshold_removal=0.002\n",
    "        \n",
    "        ff=list(Poly_feature_selection(set(X_train_Poly_dt[(key,key1)].columns), X_train_Poly_dt[(key,key1)], y_train_dt[(key,key1)], pipe, threshold_addition=threshold_addition, threshold_removal=threshold_removal,dd=35))\n",
    "        print(\"Optimal subset of features:\")\n",
    "        print(len(ff),ff)\n",
    "\n",
    "        print(compute_avg_MR2_fast(ff, X_train_Poly_dt[(key,key1)], y_train_dt[(key,key1)], pipe))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imputed Urban model\n",
      "n_components 6 alpha 80\n",
      "Processing block 0 to 50\n",
      "Processing block 50 to 100\n",
      "Processing block 100 to 150\n",
      "Processing block 150 to 200\n",
      "Processing block 200 to 250\n",
      "Processing block 250 to 300\n",
      "Processing block 300 to 350\n",
      "Processing block 350 to 400\n",
      "Processing block 400 to 450\n",
      "Processing block 450 to 500\n",
      "Optimal subset of features:\n",
      "20 ['uninsured_rate^2', 'adjusted_income public_school_rate', 'clearance_rate dropout_rate', 'adj_mental_health_budget adj_rehab_budget', 'unemployment_rate uninsured_rate', 'clearance_rate mobile_home_ratio', 'population_density adj_rehab_budget', 'adj_welfare_budget adj_mental_health_budget', 'mobile_home_ratio median_house_value', 'adj_mental_health_budget adj_health_budget', 'mobile_home_ratio median_age', 'dropout_rate', 'adj_health_budget^2', 'adj_welfare_budget house_affordability', 'poverty_rate house_affordability', 'adjusted_income vacancy_rate', 'adj_mental_health_budget dropout_rate', 'adj_police_budget adj_rehab_budget', 'unemployment_rate median_house_value', 'clearance_rate']\n",
      "0.7315007740986458\n",
      "\n",
      "\n",
      "n_components 6 alpha 100\n",
      "Processing block 0 to 50\n",
      "Processing block 50 to 100\n",
      "Processing block 100 to 150\n",
      "Processing block 150 to 200\n",
      "Processing block 200 to 250\n",
      "Processing block 250 to 300\n",
      "Processing block 300 to 350\n",
      "Processing block 350 to 400\n",
      "Processing block 400 to 450\n",
      "Processing block 450 to 500\n",
      "Optimal subset of features:\n",
      "15 ['clearance_rate median_house_value', 'poverty_rate uninsured_rate', 'renter_ratio uninsured_rate', 'home_ownership_rate', 'poverty_rate Number_of_Persons_per_HseHld', 'vacancy_rate median_age', 'adj_mental_health_budget adj_rehab_budget', 'clearance_rate median_age', 'adj_mental_health_budget', 'mobile_home_ratio', 'adj_mental_health_budget dropout_rate', 'adj_welfare_budget adj_mental_health_budget', 'adj_mental_health_budget adj_health_budget', 'mobile_home_ratio median_age', 'adj_mental_health_budget high_school_rate']\n",
      "0.7103450728021834\n",
      "\n",
      "\n",
      "n_components 6 alpha 120\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "n_components=6 must be between 0 and min(n_samples, n_features)=5 with svd_solver='covariance_eigh'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m threshold_removal\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.002\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#Uncomment this to perform feature selection again\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m features[(key,key1)]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[43mPoly_feature_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_train_Poly_dt\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkey1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_Poly_dt\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkey1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_dt\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkey1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold_addition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold_addition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold_removal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold_removal\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     17\u001b[0m ff\u001b[38;5;241m=\u001b[39mfeatures[(key,key1)]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimal subset of features:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[46], line 39\u001b[0m, in \u001b[0;36mPoly_feature_selection\u001b[1;34m(S, X, y, model, threshold_addition, threshold_removal, dd)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Start with the base features, and first remove any that are not contributing.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m S0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(feature_0)\n\u001b[1;32m---> 39\u001b[0m S0 \u001b[38;5;241m=\u001b[39m \u001b[43mremove_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mS0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold_removal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m#dd = 25\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Process candidate features in blocks of dd.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, N, dd):\n",
      "Cell \u001b[1;32mIn[16], line 166\u001b[0m, in \u001b[0;36mremove_features\u001b[1;34m(S, X, y, model, threshold)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# Accept removal if the drop in average MR2 is less than or equal to threshold.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_avg \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m current_avg \u001b[38;5;241m-\u001b[39m threshold:\n\u001b[1;32m--> 166\u001b[0m     candidate \u001b[38;5;241m=\u001b[39m \u001b[43mremove_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m     candidate_avg \u001b[38;5;241m=\u001b[39m compute_avg_MR2_fast(candidate, X, y, model)\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m candidate_avg \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m current_avg \u001b[38;5;241m-\u001b[39m threshold:\n",
      "Cell \u001b[1;32mIn[16], line 166\u001b[0m, in \u001b[0;36mremove_features\u001b[1;34m(S, X, y, model, threshold)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# Accept removal if the drop in average MR2 is less than or equal to threshold.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_avg \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m current_avg \u001b[38;5;241m-\u001b[39m threshold:\n\u001b[1;32m--> 166\u001b[0m     candidate \u001b[38;5;241m=\u001b[39m \u001b[43mremove_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m     candidate_avg \u001b[38;5;241m=\u001b[39m compute_avg_MR2_fast(candidate, X, y, model)\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m candidate_avg \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m current_avg \u001b[38;5;241m-\u001b[39m threshold:\n",
      "    \u001b[1;31m[... skipping similar frames: remove_features at line 166 (20 times)]\u001b[0m\n",
      "Cell \u001b[1;32mIn[16], line 166\u001b[0m, in \u001b[0;36mremove_features\u001b[1;34m(S, X, y, model, threshold)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# Accept removal if the drop in average MR2 is less than or equal to threshold.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_avg \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m current_avg \u001b[38;5;241m-\u001b[39m threshold:\n\u001b[1;32m--> 166\u001b[0m     candidate \u001b[38;5;241m=\u001b[39m \u001b[43mremove_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m     candidate_avg \u001b[38;5;241m=\u001b[39m compute_avg_MR2_fast(candidate, X, y, model)\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m candidate_avg \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m current_avg \u001b[38;5;241m-\u001b[39m threshold:\n",
      "Cell \u001b[1;32mIn[16], line 163\u001b[0m, in \u001b[0;36mremove_features\u001b[1;34m(S, X, y, model, threshold)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m new_set:  \u001b[38;5;66;03m# skip if empty\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 163\u001b[0m new_avg \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_avg_MR2_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# Accept removal if the drop in average MR2 is less than or equal to threshold.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_avg \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m current_avg \u001b[38;5;241m-\u001b[39m threshold:\n",
      "Cell \u001b[1;32mIn[16], line 112\u001b[0m, in \u001b[0;36mcompute_avg_MR2_fast\u001b[1;34m(S, X, y, model, n_splits, random_state)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Clone and fit the model on the current training fold\u001b[39;00m\n\u001b[0;32m    111\u001b[0m model_clone \u001b[38;5;241m=\u001b[39m clone(model)\n\u001b[1;32m--> 112\u001b[0m \u001b[43mmodel_clone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model_clone\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[0;32m    114\u001b[0m mse_model \u001b[38;5;241m=\u001b[39m mean_squared_error(y_val, y_pred)\n",
      "File \u001b[1;32mc:\\Users\\deepe\\anaconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\deepe\\anaconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\sklearn\\pipeline.py:654\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    647\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    648\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `transform_input` parameter can only be set if metadata \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    649\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouting is enabled. You can enable metadata routing using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    650\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`sklearn.set_config(enable_metadata_routing=True)`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    651\u001b[0m     )\n\u001b[0;32m    653\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m--> 654\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\deepe\\anaconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\sklearn\\pipeline.py:588\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, routed_params, raw_params)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m    582\u001b[0m step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[0;32m    583\u001b[0m     step_idx\u001b[38;5;241m=\u001b[39mstep_idx,\n\u001b[0;32m    584\u001b[0m     step_params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[0;32m    585\u001b[0m     all_params\u001b[38;5;241m=\u001b[39mraw_params,\n\u001b[0;32m    586\u001b[0m )\n\u001b[1;32m--> 588\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32mc:\\Users\\deepe\\anaconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\joblib\\memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\deepe\\anaconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\sklearn\\pipeline.py:1551\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m   1550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1551\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1552\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1553\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   1554\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m   1555\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\deepe\\anaconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\deepe\\anaconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\deepe\\anaconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:468\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    447\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 468\u001b[0m     U, S, _, X, x_is_centered, xp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m U \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m         U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n",
      "File \u001b[1;32mc:\\Users\\deepe\\anaconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:542\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;66;03m# Call different fits for either full or truncated SVD\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcovariance_eigh\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_array_api_compliant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandomized\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_truncated(X, n_components, xp)\n",
      "File \u001b[1;32mc:\\Users\\deepe\\anaconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:556\u001b[0m, in \u001b[0;36mPCA._fit_full\u001b[1;34m(self, X, n_components, xp, is_array_api_compliant)\u001b[0m\n\u001b[0;32m    552\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmle\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is only supported if n_samples >= n_features\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    554\u001b[0m         )\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n_components \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_samples, n_features):\n\u001b[1;32m--> 556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    557\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be between 0 and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    558\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin(n_samples, n_features)=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(n_samples,\u001b[38;5;250m \u001b[39mn_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    559\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvd_solver=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    560\u001b[0m     )\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_ \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mmean(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    563\u001b[0m \u001b[38;5;66;03m# When X is a scipy sparse matrix, self.mean_ is a numpy matrix, so we need\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# to transform it to a 1D array. Note that this is not the case when X\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;66;03m# is a scipy sparse array.\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;66;03m# TODO: remove the following two lines when scikit-learn only depends\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;66;03m# on scipy versions that no longer support scipy.sparse matrices.\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: n_components=6 must be between 0 and min(n_samples, n_features)=5 with svd_solver='covariance_eigh'"
     ]
    }
   ],
   "source": [
    "key=\"imputed\"\n",
    "key1=\"Urban\"\n",
    "print(key+\" \"+key1+\" model\")\n",
    "\n",
    "\n",
    "for n_components in [6,7]:\n",
    "    for alpha in [80,100,120]:\n",
    "        print(\"n_components\",n_components,\"alpha\",alpha)\n",
    "\n",
    "        pipe=Pipeline([(\"scale\", StandardScaler()),('pca',PCA(n_components=n_components,whiten=True)), (\"ridge\", Ridge(alpha))])\n",
    "        \n",
    "        threshold_addition=0.0005\n",
    "        threshold_removal=0.002\n",
    "        #Uncomment this to perform feature selection again\n",
    "        features[(key,key1)]=list(Poly_feature_selection(set(X_train_Poly_dt[(key,key1)].columns), X_train_Poly_dt[(key,key1)], y_train_dt[(key,key1)], pipe, threshold_addition=threshold_addition, threshold_removal=threshold_removal,dd=50))\n",
    "\n",
    "        ff=features[(key,key1)]\n",
    "        print(\"Optimal subset of features:\")\n",
    "        print(len(ff),ff)\n",
    "\n",
    "        print(compute_avg_MR2_fast(ff, X_train_Poly_dt[(key,key1)], y_train_dt[(key,key1)], pipe))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key=\"imputed\"\n",
    "key1=\"Urban\"\n",
    "print(key+\" \"+key1+\" model\")\n",
    "\n",
    "\n",
    "for n_components in [7]:\n",
    "    for alpha in [80,100,120]:\n",
    "        print(\"n_components\",n_components,\"alpha\",alpha)\n",
    "\n",
    "        pipe=Pipeline([(\"scale\", StandardScaler()),('pca',PCA(n_components=n_components,whiten=True)), (\"ridge\", Ridge(alpha))])\n",
    "        \n",
    "        threshold_addition=0.0005\n",
    "        threshold_removal=0.002\n",
    "        #Uncomment this to perform feature selection again\n",
    "        features[(key,key1)]=list(Poly_feature_selection(set(X_train_Poly_dt[(key,key1)].columns), X_train_Poly_dt[(key,key1)], y_train_dt[(key,key1)], pipe, threshold_addition=threshold_addition, threshold_removal=threshold_removal,dd=50))\n",
    "\n",
    "        ff=features[(key,key1)]\n",
    "        print(\"Optimal subset of features:\")\n",
    "        print(len(ff),ff)\n",
    "\n",
    "        print(compute_avg_MR2_fast(ff, X_train_Poly_dt[(key,key1)], y_train_dt[(key,key1)], pipe))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imputed Urban model\n",
      "Processing block 0 to 25\n",
      "Processing block 25 to 50\n",
      "Processing block 50 to 75\n",
      "Processing block 75 to 100\n",
      "Processing block 100 to 125\n",
      "Processing block 125 to 150\n",
      "Processing block 150 to 175\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m threshold_removal\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.002\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#Uncomment this to perform feature selection again\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m ff\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[43mPoly_feature_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_train_Poly_dt\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkey1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_Poly_dt\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkey1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train_dt\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkey1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mridge\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold_addition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold_addition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold_removal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold_removal\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#ff=features[(key,key1)]\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimal subset of features:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[18], line 49\u001b[0m, in \u001b[0;36mPoly_feature_selection\u001b[1;34m(S, X, y, model, threshold_addition, threshold_removal, dd)\u001b[0m\n\u001b[0;32m     47\u001b[0m     S0 \u001b[38;5;241m=\u001b[39m add_features(S0, block, X, y, model, threshold\u001b[38;5;241m=\u001b[39mthreshold_addition)\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# Then run removal on the updated S0 to prune any redundant features.\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m     S0 \u001b[38;5;241m=\u001b[39m \u001b[43mremove_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mS0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold_removal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# After processing all blocks, ensure that the base features remain if beneficial.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m S0 \u001b[38;5;241m=\u001b[39m add_features(S0, \u001b[38;5;28mset\u001b[39m(feature_0), X, y, model, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 35\u001b[0m, in \u001b[0;36mremove_features\u001b[1;34m(S, X, y, model, threshold)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Accept removal if the drop in average MR2 is less than or equal to threshold.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_avg \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m current_avg \u001b[38;5;241m-\u001b[39m threshold:\n\u001b[1;32m---> 35\u001b[0m     candidate \u001b[38;5;241m=\u001b[39m \u001b[43mremove_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     candidate_avg \u001b[38;5;241m=\u001b[39m compute_avg_MR2_fast(candidate, X, y, model)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m candidate_avg \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m current_avg \u001b[38;5;241m-\u001b[39m threshold:\n",
      "Cell \u001b[1;32mIn[17], line 35\u001b[0m, in \u001b[0;36mremove_features\u001b[1;34m(S, X, y, model, threshold)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Accept removal if the drop in average MR2 is less than or equal to threshold.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_avg \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m current_avg \u001b[38;5;241m-\u001b[39m threshold:\n\u001b[1;32m---> 35\u001b[0m     candidate \u001b[38;5;241m=\u001b[39m \u001b[43mremove_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     candidate_avg \u001b[38;5;241m=\u001b[39m compute_avg_MR2_fast(candidate, X, y, model)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m candidate_avg \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m current_avg \u001b[38;5;241m-\u001b[39m threshold:\n",
      "    \u001b[1;31m[... skipping similar frames: remove_features at line 35 (5 times)]\u001b[0m\n",
      "Cell \u001b[1;32mIn[17], line 35\u001b[0m, in \u001b[0;36mremove_features\u001b[1;34m(S, X, y, model, threshold)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Accept removal if the drop in average MR2 is less than or equal to threshold.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_avg \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m current_avg \u001b[38;5;241m-\u001b[39m threshold:\n\u001b[1;32m---> 35\u001b[0m     candidate \u001b[38;5;241m=\u001b[39m \u001b[43mremove_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     candidate_avg \u001b[38;5;241m=\u001b[39m compute_avg_MR2_fast(candidate, X, y, model)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m candidate_avg \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m current_avg \u001b[38;5;241m-\u001b[39m threshold:\n",
      "Cell \u001b[1;32mIn[17], line 32\u001b[0m, in \u001b[0;36mremove_features\u001b[1;34m(S, X, y, model, threshold)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m new_set:  \u001b[38;5;66;03m# skip if empty\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m new_avg \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_avg_MR2_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Accept removal if the drop in average MR2 is less than or equal to threshold.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_avg \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m current_avg \u001b[38;5;241m-\u001b[39m threshold:\n",
      "Cell \u001b[1;32mIn[16], line 103\u001b[0m, in \u001b[0;36mcompute_avg_MR2_fast\u001b[1;34m(S, X, y, model, n_splits, random_state)\u001b[0m\n\u001b[0;32m    101\u001b[0m model_clone \u001b[38;5;241m=\u001b[39m clone(model)\n\u001b[0;32m    102\u001b[0m model_clone\u001b[38;5;241m.\u001b[39mfit(X_tt, y_tt)\n\u001b[1;32m--> 103\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_clone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m mse_model \u001b[38;5;241m=\u001b[39m mean_squared_error(y_val, y_pred)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Baseline: predict the mean of y in the training fold\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\deepe\\anaconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\sklearn\\pipeline.py:787\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m    785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 787\u001b[0m         Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    790\u001b[0m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\deepe\\anaconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:332\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data_to_wrap)\u001b[38;5;241m.\u001b[39m_make(return_tuple)\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m return_tuple\n\u001b[1;32m--> 332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrap_data_with_container\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_to_wrap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\deepe\\anaconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:268\u001b[0m, in \u001b[0;36m_wrap_data_with_container\u001b[1;34m(method, data_to_wrap, original_input, estimator)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    262\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput config must be in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msorted\u001b[39m(supported_outputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdense_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    263\u001b[0m         )\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdense\u001b[39m\u001b[38;5;124m\"\u001b[39m: dense_config}\n\u001b[1;32m--> 268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_wrap_data_with_container\u001b[39m(method, data_to_wrap, original_input, estimator):\n\u001b[0;32m    269\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrap output with container based on an estimator's or global config.\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m        DataFrame.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    293\u001b[0m     output_config \u001b[38;5;241m=\u001b[39m _get_output_config(method, estimator)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "key=\"imputed\"\n",
    "key1=\"Urban\"\n",
    "print(key+\" \"+key1+\" model\")\n",
    "\n",
    "n_components = 5\n",
    "alpha=100\n",
    "models = {\n",
    "    #\"mlr\": Pipeline([(\"scale\", StandardScaler()),('pca',PCA(n_components=n_components)), (\"Lin\", LinearRegression())]),\n",
    "    #\"xgb\": xgb.XGBRegressor(random_state=42,max_depth=3, n_estimators=100),\n",
    "    \"ridge\": Pipeline([(\"scale\", StandardScaler()),('pca',PCA(n_components=n_components,whiten=True)), (\"ridge\", Ridge(alpha))]),\n",
    "    #\"random_forest\": Pipeline([(\"scale\", StandardScaler()),('pca',PCA(n_components=n_components)), (\"randomforest\", RandomForestRegressor())]),\n",
    "}\n",
    "\n",
    "threshold_addition=0.0005\n",
    "threshold_removal=0.002\n",
    "#Uncomment this to perform feature selection again\n",
    "ff=list(Poly_feature_selection(set(X_train_Poly_dt[(key,key1)].columns), X_train_Poly_dt[(key,key1)], np.log(y_train_dt[(key,key1)]), models[\"ridge\"], threshold_addition=threshold_addition, threshold_removal=threshold_removal))\n",
    "\n",
    "#ff=features[(key,key1)]\n",
    "print(\"Optimal subset of features:\")\n",
    "print(len(ff),ff)\n",
    "\n",
    "\n",
    "fitted_models[(key,key1)], models_stats[(key,key1)], models_pred[(key,key1)] = deep_cross_fit1(\n",
    "    X_train_Poly_dt[(key,key1)][ff],X_test_Poly_dt[(key,key1)][ff], np.log(y_train_dt[(key,key1)]),np.log(y_test_dt[(key,key1)]), **models\n",
    ")\n",
    "\n",
    "# Using pretty table\n",
    "print_table(models_stats[(key,key1)])\n",
    "feature_importance_PCA(fitted_models[(key,key1)],features[(key,key1)],(3,3), plot=False, models_to_use=['ridge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_spring_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
